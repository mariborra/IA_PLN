{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traducción de Español a Inglés con KerasNLP\n",
    "\n",
    "**Autoras:** Alina Rojas y Mar Iborra<br>\n",
    "**Basado en el trabajo original de:** [Abheesht Sharma](https://github.com/abheesht17/)<br>\n",
    "**Fecha de creación de la adaptación:** 2023/12/31<br>\n",
    "**Última modificación:** 2024/01/12<br>\n",
    "**Descripción:** Utilizar KerasNLP para entrenar un modelo Transformer de secuencia a secuencia en la tarea de traducción automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este proyecto utilizaremos las capas proporcionadas por KerasNLP para construir un modelo Transformer codificador-decodificador y entrenarlo en la tarea de traducción automática del español al inglés. \n",
    "\n",
    "Este proyecto se basa en el ejemplo de traducción automática de inglés a español que se encuentra en la [documentación oficial de Keras](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/), creado por [fchollet](https://twitter.com/fchollet), ejemplo adapatado también a la traducción de texto español-inglés. El ejemplo original de fchollet es más detallado y utiliza capas implementadas desde cero, mientras que en este proyecto utilizaremos las capas proporcionadas por KerasNLP, lo que facilitará la implementación de tareas más avanzadas, como la tokenización de subpalabras y el cálculo de métricas para evaluar la calidad de las traducciones generadas.\n",
    "\n",
    "Este proyecto implica:\n",
    "\n",
    "- Tokenizar texto utilizando `keras_nlp.tokenizers.WordPieceTokenizer`.\n",
    "- Implementar un modelo Transformer secuencia a secuencia utilizando las capas `keras_nlp.layers.TransformerEncoder`, `keras_nlp.layers.TransformerDecoder` y `keras_nlp.layers.TokenAndPositionEmbedding` de KerasNLP, y entrenarlo.\n",
    "- Utilizar `keras_nlp.samplers` para generar traducciones de oraciones de entrada no vistas utilizando la estrategia de decodificación \"top-p\".\n",
    "\n",
    "\n",
    "### Capas KerasNLP\n",
    "\n",
    "Las capas KerasNLP son componentes esenciales proporcionados por la biblioteca KerasNLP que simplifican la construcción de modelos de procesamiento del lenguaje natural (NLP). A diferencia de implementar estas capas desde cero, KerasNLP ofrece una interfaz de alto nivel que facilita la creación y entrenamiento de modelos NLP avanzados. Algunas de las capas clave incluyen:\n",
    "\n",
    "1. **WordPieceTokenizer**: Esta capa se utiliza para tokenizar el texto en subpalabras, lo que es especialmente útil para lidiar con palabras compuestas y idiomas con una gran cantidad de inflexiones. Simplifica la tokenización de palabras en fragmentos más pequeños.\n",
    "\n",
    "2. **TransformerEncoder**: Esta capa implementa la parte codificadora de un modelo Transformer. Transforma la secuencia de entrada en representaciones enriquecidas que capturan las relaciones entre las palabras y las subpalabras.\n",
    "\n",
    "3. **TransformerDecoder**: Esta capa es la contraparte de la capa codificadora y se utiliza para generar secuencias de salida basadas en las representaciones generadas por la capa codificadora. Es esencial para tareas de generación de texto, como la traducción automática.\n",
    "\n",
    "4. **TokenAndPositionEmbedding**: Esta capa combina la información de tokenización con información de posición para crear embeddings que se utilizan como entrada para el modelo Transformer. Ayuda al modelo a comprender tanto qué palabras están presentes como su ubicación en la secuencia.\n",
    "\n",
    "La principal ventaja de utilizar estas capas KerasNLP es la simplificación del proceso de construcción de modelos NLP complejos. En comparación con el ejemplo original [aquí](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/), donde se implementan estas capas desde cero, el uso de KerasNLP ahorra tiempo y reduce la complejidad del código, permitiendo concentrarse en la lógica de la aplicación en lugar de en la implementación detallada de las capas del modelo.\n",
    "\n",
    "Este proyecto utiliza las capas KerasNLP para implementar un modelo Transformer codificador-decodificador y entrenarlo en la tarea de traducción automática inglés-español, lo que facilita significativamente la construcción y el entrenamiento del modelo en comparación con una implementación desde cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración\n",
    "\n",
    "Antes de comenzar a implementar el proyecto, importemos todas las bibliotecas que necesitamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DezCIMLOVghn",
    "outputId": "9ff2dcbb-e228-4fe2-f1cb-dfd0432d4a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade rouge-score\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UbzSVbWOqso",
    "outputId": "cf701bc3-d73b-4c82-a50b-e6d25b8b142c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: Hemos realizado un cambio con respecto al código original en las bibliotecas utilizadas. Hemos actualizado las bibliotecas `rouge-score`, `keras-nlp` y `tensorflow` para asegurarnos de tener las versiones más recientes y compatibles con nuestro proyecto. Estas bibliotecas son esenciales para implementar y evaluar nuestro modelo Transformer de traducción automática inglés-español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos también nuestros parámetros e hiperparámetros para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k5knbnQqOqsq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "MAX_SEQUENCE_LENGTH = 40\n",
    "ENG_VOCAB_SIZE = 15000\n",
    "SPA_VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargando los Datos\n",
    "\n",
    "Trabajaremos con un conjunto de datos de traducción de inglés a español proporcionado por [Anki](https://www.manythings.org/anki/). Aunque el conjunto de datos original está diseñado para la traducción de inglés a español, adaptaremos el uso de estos datos para realizar traducciones en la dirección opuesta, de español a inglés. Descarguemos el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mN3fef7BOqss"
   },
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesando los Datos\n",
    "\n",
    "Cada línea contiene una oración en español y su correspondiente oración en inglés. La oración en inglés es la *secuencia fuente* y la española es la *secuencia objetivo*. Antes de agregar el texto a una lista, lo convertimos a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2R_gN9lkOqst"
   },
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    eng = eng.lower()\n",
    "    spa = spa.lower()\n",
    "    text_pairs.append((spa, eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto, estamos trabajando con datos de traducción de español a inglés, por lo que hemos invertido el orden de las oraciones en comparación con el conjunto de datos original, que es de inglés a español."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así es como se ven nuestras parejas de oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUulcWvDOqsx",
    "outputId": "369091b6-e613-4208-b881-0e1acc6504f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('la cámara costará al menos 500 dólares.', 'the camera will cost at least $500.')\n",
      "('casi tengo treinta años.', \"i'm almost thirty years old.\")\n",
      "('te ves más joven que tom.', 'you look younger than tom.')\n",
      "('de ser tú, yo no lo haría.', 'if i were you, i would not do it.')\n",
      "('¿cuándo es el cumpleaños de tom?', \"when is tom's birthday?\")\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a dividir las parejas de oraciones en un conjunto de entrenamiento, un conjunto de validación y un conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrCNPIkJOqsy",
    "outputId": "57c021df-7ec2-428f-d542-e979e1344657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización de los Datos\n",
    "\n",
    "Definiremos dos tokenizadores: uno para el idioma fuente (español) y otro para el idioma objetivo (inglés). Utilizaremos `keras_nlp.tokenizers.WordPieceTokenizer` para tokenizar el texto. `keras_nlp.tokenizers.WordPieceTokenizer` toma un vocabulario WordPiece y tiene funciones para tokenizar el texto y deshacer la tokenización de secuencias de tokens.\n",
    "\n",
    "Antes de definir los dos tokenizadores, primero debemos entrenarlos en el conjunto de datos que tenemos. El algoritmo de tokenización WordPiece es un algoritmo de subword tokenization; entrenarlo en un corpus nos proporciona un vocabulario de subwords. Un tokenizador de subwords es un compromiso entre los tokenizadores de palabras (los tokenizadores de palabras necesitan vocabularios muy grandes para una buena cobertura de palabras de entrada) y los tokenizadores de caracteres (los caracteres no codifican realmente el significado como lo hacen las palabras). Afortunadamente, KerasNLP facilita mucho el entrenamiento de WordPiece en un corpus con la utilidad `keras_nlp.tokenizers.compute_word_piece_vocabulary`.\n",
    "\n",
    "En este proyecto, utilizaremos estos tokenizadores para procesar nuestros datos de traducción español-inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jXpeyGrCOqsz"
   },
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada vocabulario tiene algunos tokens especiales y reservados. En nuestro caso, tenemos cuatro de estos tokens:\n",
    "\n",
    "- `\"[PAD]\"` - Token de relleno. Los tokens de relleno se agregan al final de la secuencia de entrada cuando esta es más corta que la longitud máxima de secuencia.\n",
    "- `\"[UNK]\"` - Token desconocido.\n",
    "- `\"[START]\"` - Token que marca el comienzo de la secuencia de entrada.\n",
    "- `\"[END]\"` - Token que marca el final de la secuencia de entrada.\n",
    "\n",
    "Estos tokens desempeñan roles específicos en el procesamiento de texto y son parte fundamental de nuestros tokenizadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7ey1974sOqs0"
   },
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "spa_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "eng_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver algunos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBo0ZnC7Oqs1",
    "outputId": "4e67d222-9948-42e6-e8f6-7086c29c8ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Tokens:  ['mi', 'qué', 'ella', 'le', 'te', 'para', 'mary', 'las', 'más', 'al']\n",
      "English Tokens:  ['know', 'him', 'there', 'go', 'they', 'her', 'has', 'will', 'time', 're']\n"
     ]
    }
   ],
   "source": [
    "print(\"Spanish Tokens: \", spa_vocab[100:110])\n",
    "print(\"English Tokens: \", eng_vocab[100:110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, definamos los tokenizadores. Configuraremos los tokenizadores con los vocabularios que entrenamos previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YGzkmBBSOqs1"
   },
   "outputs": [],
   "source": [
    "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=spa_vocab, lowercase=False\n",
    ")\n",
    "\n",
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos tokenizar un ejemplo de nuestro conjunto de datos. Para verificar si el texto se ha tokenizado correctamente, también podemos deshacer la tokenización y volver al texto original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09r1BtXlOqs2",
    "outputId": "7867d981-42ad-4b25-8fd4-0d362a1bdaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish sentence:  mi casa está cerca de la escuela.\n",
      "Tokens:  tf.Tensor([100 124  97 376  80  84 269  15], shape=(8,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'mi casa est\\xc3\\xa1 cerca de la escuela .', shape=(), dtype=string)\n",
      "\n",
      "English sentence:  my house is near the school.\n",
      "Tokens:  tf.Tensor([ 80 175  69 631  65 194  12], shape=(7,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'my house is near the school .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "spa_input_ex = text_pairs[0][0]\n",
    "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
    "print(\"Spanish sentence: \", spa_input_ex)\n",
    "print(\"Tokens: \", spa_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "eng_output_ex = text_pairs[0][1]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_output_ex)\n",
    "print(\"English sentence: \", eng_output_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, formatearemos nuestros conjuntos de datos.\n",
    "\n",
    "En cada paso de entrenamiento, el modelo buscará predecir las palabras objetivo N+1 (y más allá) utilizando la oración fuente y las palabras objetivo de 0 a N.\n",
    "\n",
    "Por lo tanto, el conjunto de datos de entrenamiento generará una tupla `(inputs, targets)`, donde:\n",
    "\n",
    "- `inputs` es un diccionario con las claves `encoder_inputs` y `decoder_inputs`.\n",
    "`encoder_inputs` es la oración fuente tokenizada y `decoder_inputs` es la oración objetivo \"hasta ahora\",\n",
    "es decir, las palabras de 0 a N utilizadas para predecir la palabra N+1 (y más allá) en la oración objetivo.\n",
    "- `target` es la oración objetivo desplazada en un paso:\n",
    "proporciona las siguientes palabras en la oración objetivo, es decir, lo que el modelo intentará predecir.\n",
    "\n",
    "Añadiremos tokens especiales, `\"[START]\"` y `\"[END]\"`, a la oración en español de entrada después de tokenizar el texto. También rellenaremos la entrada a una longitud fija. Esto se puede hacer fácilmente utilizando `keras_nlp.layers.StartEndPacker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6dsdVpAqOqs2"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(spa, eng):\n",
    "    # Tokenizar las oraciones en español e inglés\n",
    "    spa = spa_tokenizer(spa)\n",
    "    eng = eng_tokenizer(eng)\n",
    "\n",
    "    # Agregar tokens especiales (\"[START]\" y \"[END]\") a `eng` y padear.\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=eng_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Padear `spa` hasta `MAX_SEQUENCE_LENGTH`.\n",
    "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    spa = spa_start_end_packer(spa)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": spa,\n",
    "            \"decoder_inputs\": eng[:, :-1],\n",
    "        },\n",
    "        eng[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    spa_texts, eng_texts = zip(*pairs)\n",
    "    spa_texts = list(spa_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((spa_texts, eng_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "# Crear datasets de entrenamiento y validación\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo rápido a las formas de las secuencias (tenemos lotes de 64 pares y todas las secuencias tienen 40 pasos de longitud):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1G0BXrkyOqs3",
    "outputId": "8fc7c26f-fd42-4b2c-d215-974b58f79efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 40)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 40)\n",
      "targets.shape: (64, 40)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora pasemos a la parte emocionante: ¡definir nuestro modelo!\n",
    "Primero necesitamos una capa de inserción, es decir, un vector para cada token en nuestra secuencia de entrada. Esta capa de inserción puede inicializarse de manera aleatoria. También necesitamos una capa de inserción posicional que codifique el orden de las palabras en la secuencia. La convención es agregar estas dos inserciones. KerasNLP tiene una capa `keras_nlp.layers.TokenAndPositionEmbedding` que realiza todos los pasos mencionados anteriormente por nosotros.\n",
    "\n",
    "Nuestro modelo Transformer secuencia a secuencia consta de una capa `keras_nlp.layers.TransformerEncoder` y una capa `keras_nlp.layers.TransformerDecoder` encadenadas.\n",
    "\n",
    "La secuencia de origen se pasará a `keras_nlp.layers.TransformerEncoder`, que producirá una nueva representación de la misma. Esta nueva representación se pasará luego al `keras_nlp.layers.TransformerDecoder`, junto con la secuencia objetivo hasta ahora (palabras objetivo de 0 a N). El `keras_nlp.layers.TransformerDecoder` buscará predecir las próximas palabras en la secuencia objetivo (N+1 en adelante).\n",
    "\n",
    "Un detalle clave que hace esto posible es el enmascaramiento causal. El `keras_nlp.layers.TransformerDecoder` ve toda la secuencia de una vez, por lo que debemos asegurarnos de que solo utilice información de los tokens objetivo de 0 a N al predecir el token N+1 (de lo contrario, podría utilizar información del futuro, lo que resultaría en un modelo que no se puede utilizar en el momento de la inferencia). El enmascaramiento causal está habilitado de forma predeterminada en `keras_nlp.layers.TransformerDecoder`.\n",
    "\n",
    "También necesitamos enmascarar los tokens de relleno (`\"[PAD]\"`). Para ello, podemos configurar el argumento `mask_zero` de la capa `keras_nlp.layers.TokenAndPositionEmbedding` en True. Esto se propagará a todas las capas subsiguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BFvi-RZuOqs4"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=SPA_VOCAB_SIZE,  # Usar el tamaño del vocabulario español\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,  # Usar el tamaño del vocabulario inglés\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ENG_VOCAB_SIZE, activation=\"softmax\")(x)  # Salida en inglés\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenando nuestro modelo\n",
    "\n",
    "Utilizaremos la precisión como una forma rápida de monitorear el progreso del entrenamiento en los datos de validación. Es importante destacar que la traducción automática generalmente utiliza puntajes BLEU, así como otras métricas, en lugar de la precisión. Sin embargo, para utilizar métricas como ROUGE, BLEU, etc., deberíamos decodificar las probabilidades y generar el texto. La generación de texto es computacionalmente costosa y realizarla durante el entrenamiento no es recomendable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0mmSXWiOqs4",
    "outputId": "c76d6f88-c26e-43d9-cfe3-b25341956be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3850240   ['encoder_inputs[0][0]']      \n",
      " ng (TokenAndPositionEmbedd                                                                       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
      " formerEncoder)                                                     [0][0]']                      \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          9283992   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14449304 (55.12 MB)\n",
      "Trainable params: 14449304 (55.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo Transformer tiene tres capas principales:\r\n",
    "\r\n",
    "1. `token_and_position_embedding`: Esta capa toma las secuencias de entrada del encoder y del decoder y las transforma en representaciones vectoriales con incrustaciones de tokens y posiciones.\r\n",
    "\r\n",
    "2. `transformer_encoder`: Esta capa es la parte del codificador del modelo Transformer. Toma las representaciones vectoriales del `token_and_position_embedding` y las procesa a través de capas de atención y redes neuronales completamente conectadas.\r\n",
    "\r\n",
    "3. `model_1`: Esta capa es la parte del decodificador del modelo Transformer. Toma las secuencias de entrada del decoder y las procesa a través de capas de atención y redes neuronales completamente conectadas. Su salida es la secuencia de palabras en el idioma de destino.\r\n",
    "\r\n",
    "El modelo tiene un total de 14,449,304 parámetros entrenables y su tamaño en memoria es de aproximadamente 55.12 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HeS33jOslXyd"
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta parte del código, se compila el modelo Transformer utilizando el optimizador \"rmsprop\" como método de optimización. Además, se utiliza la función de pérdida \"sparse_categorical_crossentropy\" que es comúnmente utilizada para problemas de clasificación con múltiples clases, como la generación de texto. Como métrica de evaluación durante el entrenamiento, se utiliza \"accuracy\" para monitorear la precisión del modelo en los datos de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akEG4qyFlYAM",
    "outputId": "aaf59794-69f0-4d98-d7b7-168f42c385ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 129s 91ms/step - loss: 0.9954 - accuracy: 0.8480 - val_loss: 0.7095 - val_accuracy: 0.8814\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.6209 - accuracy: 0.8959 - val_loss: 0.5092 - val_accuracy: 0.9111\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.4898 - accuracy: 0.9140 - val_loss: 0.4395 - val_accuracy: 0.9209\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.4247 - accuracy: 0.9229 - val_loss: 0.4045 - val_accuracy: 0.9261\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.3836 - accuracy: 0.9288 - val_loss: 0.3908 - val_accuracy: 0.9289\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.3538 - accuracy: 0.9334 - val_loss: 0.3803 - val_accuracy: 0.9305\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.3302 - accuracy: 0.9369 - val_loss: 0.3812 - val_accuracy: 0.9318\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.3100 - accuracy: 0.9401 - val_loss: 0.3773 - val_accuracy: 0.9327\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.2921 - accuracy: 0.9431 - val_loss: 0.3721 - val_accuracy: 0.9342\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 96s 74ms/step - loss: 0.2765 - accuracy: 0.9456 - val_loss: 0.3745 - val_accuracy: 0.9345\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.2618 - accuracy: 0.9480 - val_loss: 0.3762 - val_accuracy: 0.9345\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.2487 - accuracy: 0.9502 - val_loss: 0.3778 - val_accuracy: 0.9350\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.2360 - accuracy: 0.9523 - val_loss: 0.3800 - val_accuracy: 0.9352\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.2250 - accuracy: 0.9542 - val_loss: 0.3832 - val_accuracy: 0.9355\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.2147 - accuracy: 0.9560 - val_loss: 0.3833 - val_accuracy: 0.9360\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.2049 - accuracy: 0.9576 - val_loss: 0.3877 - val_accuracy: 0.9360\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1967 - accuracy: 0.9590 - val_loss: 0.3915 - val_accuracy: 0.9364\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1881 - accuracy: 0.9606 - val_loss: 0.3978 - val_accuracy: 0.9365\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1804 - accuracy: 0.9621 - val_loss: 0.3979 - val_accuracy: 0.9363\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1737 - accuracy: 0.9632 - val_loss: 0.4044 - val_accuracy: 0.9361\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1673 - accuracy: 0.9644 - val_loss: 0.4085 - val_accuracy: 0.9367\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1608 - accuracy: 0.9654 - val_loss: 0.4079 - val_accuracy: 0.9371\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.1552 - accuracy: 0.9666 - val_loss: 0.4161 - val_accuracy: 0.9366\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1497 - accuracy: 0.9676 - val_loss: 0.4158 - val_accuracy: 0.9368\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1442 - accuracy: 0.9687 - val_loss: 0.4213 - val_accuracy: 0.9371\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.1396 - accuracy: 0.9695 - val_loss: 0.4236 - val_accuracy: 0.9367\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 107s 82ms/step - loss: 0.1350 - accuracy: 0.9704 - val_loss: 0.4279 - val_accuracy: 0.9371\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 97s 75ms/step - loss: 0.1308 - accuracy: 0.9712 - val_loss: 0.4317 - val_accuracy: 0.9368\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 97s 74ms/step - loss: 0.1267 - accuracy: 0.9720 - val_loss: 0.4383 - val_accuracy: 0.9368\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 98s 75ms/step - loss: 0.1229 - accuracy: 0.9728 - val_loss: 0.4417 - val_accuracy: 0.9360\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e11e069b940>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo ha sido entrenado durante 30 épocas. Durante el entrenamiento, la pérdida (loss) en los datos de entrenamiento disminuyó gradualmente, lo que indica que el modelo está aprendiendo y mejorando su capacidad para realizar traducciones. Además, la precisión (accuracy) en los datos de entrenamiento también aumentó, lo que indica que el modelo está haciendo predicciones más precisas en cada época.\r\n",
    "\r\n",
    "En los datos de validación, observamos un patrón similar de disminución de la pérdida y aumento de la precisión, lo que sugiere que el modelo generaliza bien a datos no vistos.\r\n",
    "\r\n",
    "Sin embargo, para determinar si el entrenamiento es bueno o no, sería necesario evaluar el modelo en datos de prueba independientes y medir métricas de evaluación como BLEU o ROUGE, que son más adecuadas para la evaluación de traducción automática. Estos resultados proporcionarían una evaluación más precisa del rendimiento del modelo en tareas de traducción de idiomas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificación de oraciones de prueba (análisis cualitativo)\r\n",
    "\r\n",
    "Finalmente, vamos a demostrar cómo traducir nuevas oraciones en inglés. Simplemente alimentamos al modelo la oración en inglés tokenizada, así como el token objetivo `\"[START]\"`. El modelo genera probabilidades del siguiente token. Luego, generamos repetidamente el siguiente token condicionado a los tokens generados hasta ahora, hasta que llegamos al token `\"[END]\"`.\r\n",
    "\r\n",
    "Para la decodificación, utilizaremos el módulo `keras_nlp.samplers` de KerasNLP. La Decodificación Greedy es un método de decodificación de texto que genera el token siguiente más probable en cada paso de tiempo, es decir, el token con la probabilidad más alta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAhcTge7Oqs5",
    "outputId": "f28fc045-dae6-47f1-d27c-a4324b794bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "me pareció muy divertido.\n",
      "i thought it was very fun .\n",
      "\n",
      "** Example 1 **\n",
      "yo estoy siendo buena con ustedes esta mañana.\n",
      "i ' m being good to you this morning .\n",
      "\n",
      "** Example 2 **\n",
      "a la mañana siguiente el muñeco de nieve estaba completamente derretido.\n",
      "the morning after the morningoco was completely melregr .\n",
      "\n",
      "** Example 3 **\n",
      "quería comprar el libro.\n",
      "i wanted to get the book .\n",
      "\n",
      "** Example 4 **\n",
      "ese movimiento fue un gran error.\n",
      "that motar was a big mistake .\n",
      "\n",
      "** Example 5 **\n",
      "todavía no se ha leído el libro.\n",
      "not haven ' t read the book yet .\n",
      "\n",
      "** Example 6 **\n",
      "le di tres libros de texto a cambio de su ayuda.\n",
      "he is three designed books of his help .\n",
      "\n",
      "** Example 7 **\n",
      "tom trabaja en arqueología.\n",
      "tom works intrchology .\n",
      "\n",
      "** Example 8 **\n",
      "es francés.\n",
      "french is .\n",
      "\n",
      "** Example 9 **\n",
      "estaría bien tener una de esas, ¿a que sí?\n",
      "you would be good at those who do to ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = 1\n",
    "\n",
    "    # Tokenizar la entrada del codificador (en español).\n",
    "    encoder_input_tokens = spa_tokenizer(input_sentences).to_tensor()\n",
    "\n",
    "    # Si la longitud de la secuencia es menor que MAX_SEQUENCE_LENGTH, aplicar padding.\n",
    "    sequence_length = tf.shape(encoder_input_tokens)[1]\n",
    "    if sequence_length < MAX_SEQUENCE_LENGTH:\n",
    "        pads = tf.fill([batch_size, MAX_SEQUENCE_LENGTH - sequence_length], 0)\n",
    "        encoder_input_tokens = tf.concat([encoder_input_tokens, pads], 1)\n",
    "\n",
    "    # Definir una función para la decodificación.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        return logits, None, cache\n",
    "\n",
    "    # Construir un prompt con un token de inicio y tokens de padding.\n",
    "    length = 40\n",
    "    start = tf.fill([batch_size, 1], eng_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill([batch_size, length - 1], eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat([start, pad], axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1,\n",
    "    )\n",
    "    generated_sentences = eng_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(10):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de nuestro proyecto (análisis cuantitativo)\r\n",
    "\r\n",
    "Existen muchas métricas que se utilizan para tareas de generación de texto. Aquí, para evaluar las traducciones generadas por nuestro proyecto, calcularemos los puntajes ROUGE-1 y ROUGE-2. Básicamente, ROUGE-N es un puntaje basado en el número de n-gramas comunes entre el texto de referencia y el texto generado. ROUGE-1 y ROUGE-2 utilizan el número de unigramas y bigramas comunes, respectivamente.\r\n",
    "\r\n",
    "Calcularemos el puntaje en base a 30 muestras de prueba (ya que la decodificación es un proceso costoso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAzFRW5GOqs6",
    "outputId": "ebd369b9-c0ca-4aac-a595-4cb80d5dcb30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Scores:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.6953174>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.6882178>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.6867349>}\n",
      "ROUGE-2 Scores:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.51784396>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.5083622>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.5093947>}\n"
     ]
    }
   ],
   "source": [
    "rouge_1 = keras_nlp.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_nlp.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    # Evaluar con ROUGE-1 y ROUGE-2\n",
    "    rouge_1.update_state([reference_sentence], [translated_sentence])\n",
    "    rouge_2.update_state([reference_sentence], [translated_sentence])\n",
    "\n",
    "# Obtener el diccionario completo de puntuaciones ROUGE\n",
    "rouge_1_scores = rouge_1.result()\n",
    "rouge_2_scores = rouge_2.result()\n",
    "\n",
    "# Imprimir el diccionario completo\n",
    "print(\"ROUGE-1 Scores: \", rouge_1_scores)\n",
    "print(\"ROUGE-2 Scores: \", rouge_2_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de 30 épocas, los puntajes son los siguientes:\n",
    "\n",
    "|               | **ROUGE-1** | **ROUGE-2** |\n",
    "|:-------------:|:-----------:|:-----------:|\n",
    "| **Precision** |    0.6953174    |    0.51784396    |\n",
    "|   **Recall**  |    0.6882178    |    0.5083622    |\n",
    "|  **F1 Score** |    0.6867349    |    0.5093947    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos puntajes son una medida de la calidad de las traducciones generadas por nuestro modelo. En general, los puntajes indican lo siguiente:\n",
    "\n",
    "- ROUGE-1 Precision: El 69.53% de las palabras en las traducciones generadas coinciden con las palabras en las referencias de manera precisa.\n",
    "- ROUGE-1 Recall: Se logra recuperar el 68.82% de las palabras presentes en las referencias.\n",
    "- ROUGE-1 F1 Score: Un equilibrio entre precisión y recall con un valor de 0.6867.\n",
    "\n",
    "- ROUGE-2 Precision: El 51.78% de los bigramas (pares de palabras) en las traducciones coinciden con los bigramas en las referencias de manera precisa.\n",
    "- ROUGE-2 Recall: Se recupera el 50.84% de los bigramas presentes en las referencias.\n",
    "- ROUGE-2 F1 Score: Un equilibrio entre precisión y recall para bigramas con un valor de 0.5094.\n",
    "\n",
    "En resumen, los puntajes indican que nuestro modelo logra generar traducciones con una precisión razonable, capturando una parte significativa del contenido de las referencias. Sin embargo, aún hay margen de mejora, especialmente en el caso de los bigramas (ROUGE-2), donde la precisión y el recall son más bajos en comparación con los unigramas (ROUGE-1). Esto sugiere que el modelo podría beneficiarse de mejoras en la coherencia y la fluidez de las traducciones de frases más largas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparación entre Transformer Secuencia a Secuencia y KerasNLP\n",
    "\n",
    "Al comparar los resultados obtenidos en los dos proyectos, uno utilizando un enfoque tradicional de Transformer secuencia a secuencia y el otro empleando KerasNLP, podemos observar varias diferencias y similitudes en términos de rendimiento, calidad de las traducciones y las características inherentes de cada enfoque:\n",
    "\n",
    "1. **Calidad de la Traducción**:\n",
    "   - **Transformer Secuencia a Secuencia Tradicional**: Las traducciones generadas por este modelo muestran una comprensión básica de la estructura del lenguaje, pero con ciertas limitaciones en términos de precisión y fluidez. Algunas frases son traducidas de manera coherente, mientras que otras presentan errores gramaticales o de interpretación.\n",
    "   - **Usando KerasNLP**: Las traducciones producidas con KerasNLP parecen ser ligeramente más fluidas y coherentes. Aunque también hay errores, especialmente en frases más complejas, el grado de naturalidad en las traducciones es notable.\n",
    "\n",
    "2. **Manejo de Vocabulario y Estructuras Gramaticales**:\n",
    "   - En el modelo de Transformer secuencia a secuencia tradicional, hay casos en los que el modelo lucha con estructuras gramaticales complejas o vocabulario específico.\n",
    "   - Con KerasNLP, parece haber un mejor manejo de la diversidad lingüística, posiblemente debido a una tokenización más eficiente y un manejo más robusto del vocabulario.\n",
    "\n",
    "3. **Coherencia y Contexto**:\n",
    "   - Ambos modelos enfrentan desafíos en mantener la coherencia y capturar el contexto completo de las oraciones, pero el uso de KerasNLP puede ofrecer una ligera ventaja en términos de coherencia contextual, probablemente debido a su enfoque en las capas Transformer optimizadas.\n",
    "\n",
    "4. **Facilidad de Implementación y Flexibilidad**:\n",
    "   - KerasNLP ofrece una implementación más sencilla y modular, lo que puede acelerar el desarrollo y permitir una mayor experimentación con diferentes configuraciones de capas y parámetros.\n",
    "\n",
    "5. **Generalización y Adaptabilidad**:\n",
    "   - Mientras que el enfoque tradicional de Transformer secuencia a secuencia proporciona una base sólida, KerasNLP, con su enfoque modular y optimizado, puede ofrecer una mejor adaptabilidad a diferentes tipos de textos y tareas de traducción.\n",
    "\n",
    "En conclusión, aunque ambos enfoques tienen sus méritos y limitaciones, el uso de KerasNLP puede ofrecer ventajas en términos de facilidad de implementación, manejo de vocabulario y coherencia en las traducciones. Sin embargo, la elección entre estos enfoques dependerá de los objetivos específicos del proyecto, la complejidad de las tareas y las preferencias en cuanto a personalización y control sobre el modelo."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
