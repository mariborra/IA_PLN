{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM168afVOqsb"
   },
   "source": [
    "# Traducción de Texto Generado por RNN a Inglés con KerasNLP\n",
    "\n",
    "**Autoras:** Alina Rojas y Mar Iborra<br>\n",
    "**Basado en el trabajo original de:** [Abheesht Sharma](https://github.com/abheesht17/)<br>\n",
    "**Fecha de creación de la adaptación:** 2023/01/11<br>\n",
    "**Última modificación:** 2024/01/12<br>\n",
    "**Descripción:**  Este proyecto integra un modelo de red neuronal recurrente (RNN) ya entrenado para la generación de texto en español, basado en el estilo de Ibai Llanos, y un modelo Transformer para su traducción al inglés, utilizando KerasNLP, ya analizado anteriormente. El objetivo es demostrar la capacidad de generar y traducir texto automáticamente, combinando ambos modelos en un flujo de trabajo de procesamiento de lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En la primera parte de este proyecto, hemos desarrollado y entrenado un modelo de red neuronal recurrente (RNN) diseñado para imitar el estilo de Ibai Llanos en la generación de texto en español. Este modelo, ya entrenado, se encuentra disponible para su uso en el presente notebook, permitiendo generar automáticamente texto en español.\n",
    "\n",
    "En las siguientes fases del proyecto, nos centramos en la tarea de traducción automática de textos del español al inglés. Para ello, comparamos dos enfoques: un modelo Transformer secuencia a secuencia tradicional y otro que emplea las avanzadas capas proporcionadas por KerasNLP. La segunda y tercera parte del proyecto se dedican a analizar y contrastar estos dos modelos, evaluando sus capacidades y rendimientos.\n",
    "\n",
    "Tras un análisis exhaustivo, optamos por utilizar el modelo de traducción basado en KerasNLP. Esta decisión se fundamenta en los mejores resultados obtenidos con este enfoque, especialmente en términos de precisión y coherencia en la traducción de textos.\n",
    "\n",
    "En este notebook, combinamos el trabajo realizado en las fases anteriores del proyecto. Generamos pequeños textos en español utilizando nuestro modelo RNN ya entrenado y, a continuación, los traducimos al inglés utilizando el eficiente modelo de traducción secuencia a secuencia basado en KerasNLP. Este enfoque integrado demuestra cómo se pueden unir distintos modelos de aprendizaje profundo para crear una solución completa de procesamiento de lenguaje natural, capaz de generar y traducir textos de manera automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5jdFY59OEj9"
   },
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DezCIMLOVghn",
    "outputId": "8ed4bfe9-1fc1-4809-90c7-fde895ec6459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade rouge-score\n",
    "!pip install -q --upgrade keras-nlp\n",
    "!pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UbzSVbWOqso",
    "outputId": "dbd05f60-fa31-4ff7-8f1a-f0e21c7afc06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definamos los parámetros e hiperparámetros para el proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "k5knbnQqOqsq"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "ENG_VOCAB_SIZE = 15000\n",
    "SPA_VOCAB_SIZE = 15000\n",
    "\n",
    "EMBED_DIM = 256\n",
    "INTERMEDIATE_DIM = 2048\n",
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKcI_q27fqQ_"
   },
   "source": [
    "## Descargando los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mN3fef7BOqss"
   },
   "outputs": [],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_9RiXp6OEkG"
   },
   "source": [
    "## Procesando los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2R_gN9lkOqst"
   },
   "outputs": [],
   "source": [
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    eng = eng.lower()\n",
    "    spa = spa.lower()\n",
    "    text_pairs.append((spa, eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así es como se ven nuestras parejas de oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUulcWvDOqsx",
    "outputId": "2789462b-69e9-450c-8eda-c8f860011336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en cuanto dejó de llover, apareció un bonito arco iris.', 'as soon as it stopped raining a beautiful rainbow appeared.')\n",
      "('hicieron blanco tres veces.', 'they hit the mark three times.')\n",
      "('¿puedes comer con palillos chinos?', 'do you know how to eat with chopsticks?')\n",
      "('el profesor me advirtió que no lo hiciera otra vez.', 'my teacher warned me not to do it again.')\n",
      "('por favor ten paciencia.', 'please be patient.')\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrCNPIkJOqsy",
    "outputId": "e1114fa2-a97e-4c92-f7cd-a33cc2af1947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5xHAvakOEkJ"
   },
   "source": [
    "## Tokenización de los Datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jXpeyGrCOqsz"
   },
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7ey1974sOqs0"
   },
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "spa_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "eng_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a ver algunos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBo0ZnC7Oqs1",
    "outputId": "4ad45ca1-806d-4100-8136-e297bf7f60d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish Tokens:  ['con', 'qué', 'le', 'ella', 'te', 'para', 'mary', 'las', 'más', 'al']\n",
      "English Tokens:  ['know', 'him', 'there', 'they', 'go', 'her', 'has', 're', 'will', 'll']\n"
     ]
    }
   ],
   "source": [
    "print(\"Spanish Tokens: \", spa_vocab[100:110])\n",
    "print(\"English Tokens: \", eng_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YGzkmBBSOqs1"
   },
   "outputs": [],
   "source": [
    "spa_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=spa_vocab, lowercase=False\n",
    ")\n",
    "\n",
    "eng_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09r1BtXlOqs2",
    "outputId": "bbd187a9-d7a8-4c3e-fef6-1cd987932250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish sentence:  ni puedo ir, ni quiero.\n",
      "Tokens:  tf.Tensor([298 129 141  13 298 122  15], shape=(7,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b'ni puedo ir , ni quiero .', shape=(), dtype=string)\n",
      "\n",
      "English sentence:  i can't go, nor do i want to.\n",
      "Tokens:  tf.Tensor([  35   87    8   46  104   10 1762   76   35   95   66   12], shape=(12,), dtype=int32)\n",
      "Recovered text after detokenizing:  tf.Tensor(b\"i can ' t go , nor do i want to .\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "spa_input_ex = text_pairs[0][0]\n",
    "spa_tokens_ex = spa_tokenizer.tokenize(spa_input_ex)\n",
    "print(\"Spanish sentence: \", spa_input_ex)\n",
    "print(\"Tokens: \", spa_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    spa_tokenizer.detokenize(spa_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "eng_output_ex = text_pairs[0][1]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_output_ex)\n",
    "print(\"English sentence: \", eng_output_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "6dsdVpAqOqs2"
   },
   "outputs": [],
   "source": [
    "def preprocess_batch(spa, eng):\n",
    "    # Tokenizar las oraciones en español e inglés\n",
    "    spa = spa_tokenizer(spa)\n",
    "    eng = eng_tokenizer(eng)\n",
    "\n",
    "    # Agregar tokens especiales (\"[START]\" y \"[END]\") a `eng` y padear.\n",
    "    eng_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=eng_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Padear `spa` hasta `MAX_SEQUENCE_LENGTH`.\n",
    "    spa_start_end_packer = keras_nlp.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=spa_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    spa = spa_start_end_packer(spa)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": spa,\n",
    "            \"decoder_inputs\": eng[:, :-1],\n",
    "        },\n",
    "        eng[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    spa_texts, eng_texts = zip(*pairs)\n",
    "    spa_texts = list(spa_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((spa_texts, eng_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "# Crear datasets de entrenamiento y validación\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHeAP6iGOEkN"
   },
   "source": [
    "Echemos un vistazo rápido a las formas de las secuencias (tenemos lotes de 64 pares y todas las secuencias tienen 40 pasos de longitud):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1G0BXrkyOqs3",
    "outputId": "fa7af05b-69eb-425f-dea1-3227dae46500"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 50)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 50)\n",
      "targets.shape: (64, 50)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BFvi-RZuOqs4"
   },
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=SPA_VOCAB_SIZE,  # Usar el tamaño del vocabulario español\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_nlp.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_nlp.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,  # Usar el tamaño del vocabulario inglés\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_nlp.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ENG_VOCAB_SIZE, activation=\"softmax\")(x)  # Salida en inglés\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-EVb1i2OEkO"
   },
   "source": [
    "## Entrenando nuestro modelo\n",
    "\n",
    "Utilizaremos la precisión como una forma rápida de monitorear el progreso del entrenamiento en los datos de validación. Es importante destacar que la traducción automática generalmente utiliza puntajes BLEU, así como otras métricas, en lugar de la precisión. Sin embargo, para utilizar métricas como ROUGE, BLEU, etc., deberíamos decodificar las probabilidades y generar el texto. La generación de texto es computacionalmente costosa y realizarla durante el entrenamiento no es recomendable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F0mmSXWiOqs4",
    "outputId": "c00b2eeb-5620-49ae-909c-d2e0cab9828c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " token_and_position_embeddi  (None, None, 256)            3852800   ['encoder_inputs[0][0]']      \n",
      " ng (TokenAndPositionEmbedd                                                                       \n",
      " ing)                                                                                             \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            1315072   ['token_and_position_embedding\n",
      " formerEncoder)                                                     [0][0]']                      \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          9286552   ['decoder_inputs[0][0]',      \n",
      "                                                                     'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14454424 (55.14 MB)\n",
      "Trainable params: 14454424 (55.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "HeS33jOslXyd"
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "akEG4qyFlYAM",
    "outputId": "4cf6ae1f-dcd6-41ed-c8f4-8621186461ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 129s 92ms/step - loss: 0.8278 - accuracy: 0.8753 - val_loss: 0.5931 - val_accuracy: 0.9005\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 114s 88ms/step - loss: 0.5138 - accuracy: 0.9134 - val_loss: 0.4160 - val_accuracy: 0.9275\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 115s 88ms/step - loss: 0.4012 - accuracy: 0.9292 - val_loss: 0.3559 - val_accuracy: 0.9359\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 115s 88ms/step - loss: 0.3459 - accuracy: 0.9367 - val_loss: 0.3247 - val_accuracy: 0.9406\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.3111 - accuracy: 0.9416 - val_loss: 0.3093 - val_accuracy: 0.9431\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 119s 91ms/step - loss: 0.2860 - accuracy: 0.9454 - val_loss: 0.3063 - val_accuracy: 0.9443\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.2663 - accuracy: 0.9484 - val_loss: 0.2981 - val_accuracy: 0.9458\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.2493 - accuracy: 0.9511 - val_loss: 0.2937 - val_accuracy: 0.9471\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.2345 - accuracy: 0.9536 - val_loss: 0.2943 - val_accuracy: 0.9476\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.2213 - accuracy: 0.9557 - val_loss: 0.2924 - val_accuracy: 0.9484\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.2092 - accuracy: 0.9577 - val_loss: 0.2922 - val_accuracy: 0.9489\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1984 - accuracy: 0.9595 - val_loss: 0.2943 - val_accuracy: 0.9490\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1880 - accuracy: 0.9613 - val_loss: 0.2965 - val_accuracy: 0.9494\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1785 - accuracy: 0.9629 - val_loss: 0.3051 - val_accuracy: 0.9492\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1697 - accuracy: 0.9644 - val_loss: 0.3063 - val_accuracy: 0.9491\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 124s 95ms/step - loss: 0.1619 - accuracy: 0.9658 - val_loss: 0.3014 - val_accuracy: 0.9498\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 115s 89ms/step - loss: 0.1542 - accuracy: 0.9671 - val_loss: 0.3064 - val_accuracy: 0.9492\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 125s 96ms/step - loss: 0.1472 - accuracy: 0.9683 - val_loss: 0.3059 - val_accuracy: 0.9500\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1404 - accuracy: 0.9696 - val_loss: 0.3110 - val_accuracy: 0.9500\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1344 - accuracy: 0.9706 - val_loss: 0.3124 - val_accuracy: 0.9501\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1289 - accuracy: 0.9717 - val_loss: 0.3190 - val_accuracy: 0.9499\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1233 - accuracy: 0.9726 - val_loss: 0.3178 - val_accuracy: 0.9503\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1184 - accuracy: 0.9736 - val_loss: 0.3209 - val_accuracy: 0.9502\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 118s 90ms/step - loss: 0.1135 - accuracy: 0.9745 - val_loss: 0.3230 - val_accuracy: 0.9505\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1094 - accuracy: 0.9753 - val_loss: 0.3279 - val_accuracy: 0.9499\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.1053 - accuracy: 0.9762 - val_loss: 0.3324 - val_accuracy: 0.9505\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 116s 89ms/step - loss: 0.1012 - accuracy: 0.9769 - val_loss: 0.3337 - val_accuracy: 0.9504\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 117s 90ms/step - loss: 0.0975 - accuracy: 0.9776 - val_loss: 0.3365 - val_accuracy: 0.9503\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 125s 96ms/step - loss: 0.0940 - accuracy: 0.9783 - val_loss: 0.3402 - val_accuracy: 0.9505\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 118s 90ms/step - loss: 0.0911 - accuracy: 0.9790 - val_loss: 0.3459 - val_accuracy: 0.9503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b3a1e977ee0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación y Traducción de Texto\n",
    "\n",
    "En esta sección del proyecto, procedemos a traducir los textos generados por nuestro modelo RNN que imita el estilo discursivo de Ibai Llanos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comenzamos cargando el modelo RNN previamente entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "NZWcVjiFOQDe"
   },
   "outputs": [],
   "source": [
    "ibai_model = tf.saved_model.load('./one_step_ibai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzjDz4W8ULwI"
   },
   "source": [
    "#### Función para Generar Texto con el Modelo RNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `generate_text` es un componente clave en nuestro proyecto, diseñada para generar texto utilizando el modelo RNN previamente entrenado. Esta función se enfoca en la creación de texto en el estilo de Ibai Llanos, a partir de una cadena de inicio dada. A continuación, se desglosan sus características y funcionamiento:\n",
    "\n",
    "- **Inicialización**: La función comienza inicializando los estados del modelo RNN y estableciendo el texto de inicio. Esto se realiza mediante `tf.constant([start_string])`, donde `start_string` es una cadena proporcionada por el usuario que guiará la dirección del texto generado.\n",
    "\n",
    "- **Generación de Caracteres**: A través de un bucle `for`, la función genera caracteres uno a uno, hasta alcanzar el número deseado (`num_generate`). En cada iteración, se llama al método `generate_one_step` del modelo RNN, que predice el siguiente carácter basándose en el carácter actual y el estado del modelo.\n",
    "\n",
    "- **Construcción del Texto**: Los caracteres generados se van acumulando en la lista `result`. Una vez completada la generación, estos caracteres se unen utilizando `tf.strings.join(result)`, formando una cadena de texto completa.\n",
    "\n",
    "- **Decodificación y Resultado Final**: El texto generado, aún en formato de tensor, se decodifica a una cadena UTF-8 utilizando `.numpy().decode(\"utf-8\")`, lo que permite visualizar el resultado final en un formato legible y coherente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7RETzMDmO7f8"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, num_generate=30):\n",
    "    # Inicializa los estados del modelo y el texto de inicio.\n",
    "    states = None\n",
    "    next_char = tf.constant([start_string])\n",
    "    result = [next_char]\n",
    "\n",
    "    # Genera caracteres hasta alcanzar el límite num_generate.\n",
    "    for n in range(num_generate):\n",
    "        next_char, states = model.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "\n",
    "    # Une los caracteres generados y decodifica el resultado.\n",
    "    return tf.strings.join(result)[0].numpy().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bakvoIqYUQDk"
   },
   "source": [
    "#### Función para Traducir el Texto Generado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `decode_sequences` traduce el texto generado por el modelo RNN del español al inglés mediante el uso del modelo Transformer implementado con KerasNLP. Esta función encapsula el proceso de traducción automática, y sus principales características son:\n",
    "\n",
    "- **Tokenización de Entrada**: La función comienza tokenizando las oraciones de entrada en español. Utiliza `spa_tokenizer(input_sentences).to_tensor()` para convertir las oraciones en tokens que el modelo Transformer puede procesar.\n",
    "\n",
    "- **Aplicación de Padding**: Para garantizar que todas las secuencias tengan la misma longitud, se aplica padding a las secuencias más cortas que `MAX_SEQUENCE_LENGTH`. Esto se logra mediante el uso de `tf.fill` y `tf.concat`.\n",
    "\n",
    "- **Función de Decodificación**: Se define una función interna `next` que calcula los logits para el siguiente token en la secuencia, basándose en la entrada del codificador y el prompt actual. Este paso es crucial para determinar el siguiente token más probable en la secuencia de salida.\n",
    "\n",
    "- **Construcción de Prompt**: Se construye un prompt inicial que incluye un token de inicio y tokens de padding para guiar la generación de la secuencia de salida en inglés. Esto se hace mediante `tf.fill` y concatenando el token de inicio y los tokens de padding.\n",
    "\n",
    "- **Generación de Tokens con Sampler**: Se utiliza un `GreedySampler` de KerasNLP para generar tokens uno a uno en la secuencia de salida. Este sampler elige el token más probable en cada paso, construyendo así la traducción final.\n",
    "\n",
    "- **Detokenización y Resultado Final**: Finalmente, los tokens generados se convierten de nuevo en texto legible mediante el `detokenize` del tokenizador de inglés, proporcionando así la traducción final en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "eSN7B18iQuQB"
   },
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = len(input_sentences)\n",
    "\n",
    "    # Tokenizar la entrada del codificador (en español).\n",
    "    encoder_input_tokens = spa_tokenizer(input_sentences).to_tensor()\n",
    "\n",
    "    # Aplicar padding si la longitud de la secuencia es menor que MAX_SEQUENCE_LENGTH.\n",
    "    sequence_length = tf.shape(encoder_input_tokens)[1]\n",
    "    if sequence_length < MAX_SEQUENCE_LENGTH:\n",
    "        pads = tf.fill([batch_size, MAX_SEQUENCE_LENGTH - sequence_length], 0)\n",
    "        encoder_input_tokens = tf.concat([encoder_input_tokens, pads], 1)\n",
    "\n",
    "    # Definir una función para la decodificación.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        return logits, None, cache\n",
    "\n",
    "    # Construir un prompt con un token de inicio y tokens de padding.\n",
    "    length = 40\n",
    "    start = tf.fill([batch_size, 1], eng_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill([batch_size, length - 1], eng_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat([start, pad], axis=-1)\n",
    "\n",
    "    # Usar el sampler para generar tokens.\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=eng_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1,\n",
    "    )\n",
    "    generated_sentences = eng_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQRF7z1wUTp1"
   },
   "source": [
    "#### Iterar sobre las Frases Iniciales y Realizar la Generación y Traducción:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección del proyecto, nos enfocamos en la generación y traducción de texto, empleando tanto el modelo RNN entrenado como el modelo de traducción Transformer. La metodología utilizada se desglosa en los siguientes pasos:\n",
    "\n",
    "1. **Selección de Frases Iniciales**: Se define una lista de `start_strings`, que son frases en español utilizadas como punto de partida para la generación de texto. Estas frases iniciales varían en contenido y estructura, permitiendo explorar la diversidad en la generación del modelo RNN.\n",
    "\n",
    "2. **Generación de Texto en Español**: Para cada frase inicial, se invoca la función `generate_text` con el modelo RNN (`ibai_model`) y la frase de inicio. Esta función produce una secuencia de texto en español que refleja el estilo y tono del modelo RNN entrenado. Se imprime el texto generado para cada frase inicial, permitiendo observar la capacidad del modelo RNN para crear texto coherente y contextualmente relevante.\n",
    "\n",
    "3. **Traducción al Inglés**: Cada texto generado en español es posteriormente traducido al inglés utilizando la función `decode_sequences`. Esta función se apoya en el modelo Transformer para convertir las secuencias de texto español en su equivalente en inglés. El proceso de traducción considera la coherencia gramatical y semántica, buscando generar traducciones precisas y fluidas.\n",
    "\n",
    "4. **Presentación de Resultados**: Tras la traducción, se presenta el texto original en español junto con su traducción al inglés. Este proceso se repite para cada una de las frases iniciales, proporcionando una variedad de ejemplos que demuestran la efectividad del modelo RNN en la generación de texto y la capacidad del modelo Transformer para traducir el texto generado.\n",
    "\n",
    "Este enfoque iterativo de generación y traducción de texto ilustra la sinergia entre dos modelos de aprendizaje profundo avanzados. A través de este proceso, se explora cómo la inteligencia artificial puede no solo crear contenido lingüístico, sino también traducirlo a otro idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chcOGJSuQw6A",
    "outputId": "7bc0a323-1d9f-4dea-a697-c4590b577afa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando y traduciendo para: '¿ te acuerdas'\n",
      "Texto generado en Español:\n",
      "¿ te acuerdas ? algunos sí quedo y que lo q\n",
      "\n",
      "Texto traducido al Inglés:\n",
      "do you remember any longer . to stay .\n",
      "--------------------------------------------------\n",
      "Generando y traduciendo para: 'hola, buenas'\n",
      "Texto generado en Español:\n",
      "hola, buenas tardes seguirás, se ha ido co\n",
      "\n",
      "Texto traducido al Inglés:\n",
      "hi , you ' ll be late , never gone .\n",
      "--------------------------------------------------\n",
      "Generando y traduciendo para: 'mi nombre'\n",
      "Texto generado en Español:\n",
      "mi nombre de cosco, bueno, siré lo que \n",
      "\n",
      "Texto traducido al Inglés:\n",
      "my name is hiding , if you , i ' ll give it .\n",
      "--------------------------------------------------\n",
      "Generando y traduciendo para: '¿ tú crees que'\n",
      "Texto generado en Español:\n",
      "¿ tú crees que es messi ? ahora hablan del c\n",
      "\n",
      "Texto traducido al Inglés:\n",
      "do you think he is now ?\n",
      "--------------------------------------------------\n",
      "Generando y traduciendo para: '¿ que no hay'\n",
      "Texto generado en Español:\n",
      "¿ que no hay huevos ? ahora.  ahora, ahora\n",
      "\n",
      "Texto traducido al Inglés:\n",
      "since is there any eggs now ?\n",
      "--------------------------------------------------\n",
      "Generando y traduciendo para: 'o sea, que'\n",
      "Texto generado en Español:\n",
      "o sea, que  como ven en la tele y un día\n",
      "\n",
      "Texto traducido al Inglés:\n",
      "come on to watch , but on tv a day .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "start_strings = [\"¿ te acuerdas\", \"hola, buenas\", \"mi nombre\", \"¿ tú crees que\", \"¿ que no hay\", \"o sea, que\"]\n",
    "\n",
    "for start_string in start_strings:\n",
    "    print(f\"Generando y traduciendo para: '{start_string}'\")\n",
    "\n",
    "    # Generar texto en español\n",
    "    generated_text = generate_text(ibai_model, start_string)\n",
    "    print(\"Texto generado en Español:\")\n",
    "    print(generated_text)\n",
    "    print()\n",
    "\n",
    "    # Traducir al inglés\n",
    "    translated_text = decode_sequences([generated_text])\n",
    "    translated_text = translated_text.numpy()[0].decode(\"utf-8\").replace(\"[PAD]\", \"\").replace(\"[START]\", \"\").replace(\"[END]\", \"\").strip()\n",
    "    print(\"Texto traducido al Inglés:\")\n",
    "    print(translated_text)\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Resultados de Generación y Traducción de Texto:\n",
    "\n",
    "A continuación, se presenta un análisis detallado de los resultados obtenidos en la generación y traducción de texto utilizando el modelo RNN y el modelo Transformer:\n",
    "\n",
    "1. **Frase Inicial: '¿ te acuerdas'**\n",
    "   - **Texto en Español**: \"¿ te acuerdas ? algunos sí quedo y que lo q\"\n",
    "   - **Traducción al Inglés**: \"do you remember any longer . to stay .\"\n",
    "   - **Análisis**: La generación en español capta la esencia interrogativa de la frase inicial, pero la continuidad del texto parece incompleta. La traducción al inglés mantiene la pregunta inicial, pero añade una segunda parte que parece desvinculada del contexto.\n",
    "\n",
    "2. **Frase Inicial: 'hola, buenas'**\n",
    "   - **Texto en Español**: \"hola, buenas tardes seguirás, se ha ido co\"\n",
    "   - **Traducción al Inglés**: \"hi , you ' ll be late , never gone .\"\n",
    "   - **Análisis**: El texto generado en español comienza con un saludo y parece iniciar una conversación, pero termina abruptamente. La traducción al inglés intenta seguir el hilo, aunque presenta una conclusión poco clara.\n",
    "\n",
    "3. **Frase Inicial: 'mi nombre'**\n",
    "   - **Texto en Español**: \"mi nombre de cosco, bueno, siré lo que\"\n",
    "   - **Traducción al Inglés**: \"my name is hiding , if you , i ' ll give it .\"\n",
    "   - **Análisis**: El texto en español comienza con una declaración sobre un nombre, pero no se desarrolla completamente. La traducción al inglés agrega elementos que no están presentes en el original, como \"hiding\" y \"i ' ll give it\", lo que indica una interpretación libre del modelo.\n",
    "\n",
    "4. **Frase Inicial: '¿ tú crees que'**\n",
    "   - **Texto en Español**: \"¿ tú crees que es messi ? ahora hablan del c\"\n",
    "   - **Traducción al Inglés**: \"do you think he is now ?\"\n",
    "   - **Análisis**: La generación en español formula una pregunta sobre una figura conocida (Messi) y sugiere un cambio de tema, pero no se completa. La traducción al inglés captura la pregunta pero omite el cambio de tema.\n",
    "\n",
    "5. **Frase Inicial: '¿ que no hay'**\n",
    "   - **Texto en Español**: \"¿ que no hay huevos ? ahora.  ahora, ahora\"\n",
    "   - **Traducción al Inglés**: \"since is there any eggs now ?\"\n",
    "   - **Análisis**: El texto español presenta una pregunta retórica seguida de repetición. La traducción al inglés intenta mantener la pregunta, pero introduce una palabra (\"since\") que altera el sentido original.\n",
    "\n",
    "6. **Frase Inicial: 'o sea, que'**\n",
    "   - **Texto en Español**: \"o sea, que  como ven en la tele y un día\"\n",
    "   - **Traducción al Inglés**: \"come on to watch , but on tv a day .\"\n",
    "   - **Análisis**: La generación en español comienza una reflexión sobre lo que se ve en la televisión, pero no se desarrolla completamente. La traducción al inglés intenta seguir el contexto, pero agrega elementos que no están en el texto original.\n",
    "\n",
    "**Conclusión General del resultado**: Los resultados de la generación y traducción de texto demuestran la capacidad del modelo RNN para generar texto coherente y relevante en español, aunque con ciertas limitaciones en la continuidad y coherencia. Por otro lado, el modelo Transformer logra traducir el texto generado al inglés, manteniendo en gran medida el sentido original, aunque en ocasiones introduce elementos que no se alinean completamente con el texto en español. Estos resultados reflejan la complejidad y los desafíos inherentes a la generación automática de texto y su traducción en un contexto de inteligencia artificial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión Detallada del Proyecto de Generación y Traducción de Texto\n",
    "\n",
    "#### Evaluación General del Proyecto\n",
    "Este proyecto ha integrado dos componentes clave de la inteligencia artificial en procesamiento de lenguaje natural: la generación de texto utilizando un modelo RNN y la traducción automática con un modelo Transformer. La combinación de estos modelos ha permitido no solo generar texto en español, sino también traducirlo al inglés, demostrando así la versatilidad y capacidad de la IA para abordar tareas lingüísticas complejas.\n",
    "\n",
    "#### Análisis del Modelo RNN\n",
    "El modelo RNN, entrenado para imitar el estilo de Ibai Llanos, ha mostrado una habilidad notable para generar texto que refleja patrones de habla y expresiones típicas del personaje. Sin embargo, se observan limitaciones en términos de coherencia y continuidad en los textos generados, una característica común en los modelos de generación de texto basados en caracteres. La generación de texto, aunque creativa y variada, a menudo resulta en fragmentos que carecen de un hilo conductor claro, lo que puede atribuirse a la naturaleza intrínseca del modelo RNN y a la complejidad de emular fielmente el discurso humano.\n",
    "\n",
    "#### Evaluación del Modelo Transformer para Traducción\n",
    "El modelo Transformer, implementado para la traducción del texto generado, ha demostrado ser efectivo en la conversión del español al inglés. A pesar de algunas divergencias y adiciones en la traducción que no se encontraban en el texto original, el modelo ha mantenido en su mayoría la esencia y el significado de los textos generados. Esto indica una buena comprensión del contexto y una capacidad notable para manejar la ambigüedad y la variabilidad del lenguaje natural.\n",
    "\n",
    "#### Desafíos y Limitaciones\n",
    "Un desafío notable en este proyecto ha sido equilibrar la creatividad y la coherencia en la generación de texto. Mientras que el modelo RNN ha sido capaz de producir texto creativo y diverso, mantener una narrativa coherente y lógica sigue siendo una tarea difícil. Además, la traducción automática, aunque generalmente precisa, a veces introduce elementos que no se alinean completamente con el texto original, lo que puede atribuirse a las limitaciones inherentes a los modelos de traducción automática y a que el texto generado no sea muy coherente o no se acaben las oraciones.\n",
    "\n",
    "#### Implicaciones y Futuras Direcciones\n",
    "Los resultados obtenidos abren puertas a exploraciones futuras en la generación de texto y la traducción automática. Existe un amplio margen para mejorar la coherencia y precisión en ambos modelos, posiblemente a través de un entrenamiento más extenso, la utilización de conjuntos de datos más grandes y variados, o la exploración de modelos más avanzados como GPT-3 para la generación de texto. Además, la aplicación de este enfoque en diferentes contextos y estilos de lenguaje podría proporcionar insights valiosos sobre la adaptabilidad y versatilidad de estos modelos de IA.\n",
    "\n",
    "En conclusión, este proyecto demuestra el potencial y los desafíos de la IA en el campo del procesamiento de lenguaje natural, particularmente en la generación y traducción de texto. Aunque aún hay margen de mejora, los resultados actuales son prometedores e invitan a seguir investigando posibles mejoras."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
