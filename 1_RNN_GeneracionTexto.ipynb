{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de Texto en Español Imitando el Estilo de Ibai Llanos con RNN y Keras\n",
    "\n",
    "**Autoras:** Alina Rojas y Mar Iborra<br>\n",
    "**Basado en el tutorial de TensorFlow:** [Generación de texto](https://www.tensorflow.org/text/tutorials/text_generation?hl=es-419)<br>\n",
    "**Fuente del Dataset:** [Aprende Machine Learning](https://www.aprendemachinelearning.com/) - Dataset de Ibai Llanos<br>\n",
    "**Fecha de creación de la adaptación:** 2023/12/31<br>\n",
    "**Última modificación:** 2024/01/12<br>\n",
    "**Descripción:** Este proyecto se centra en el desarrollo de un modelo de Red Neuronal Recurrente (RNN) utilizando Keras, con el objetivo de generar texto en español que imite el estilo de comunicación del famoso streamer Ibai Llanos. Inspirado en la tutorial de generación de texto de TensorFlow, este modelo se entrena con transcripciones de diálogos y entrevistas de Ibai, proporcionados por Aprende Machine Learning. El propósito es explorar las capacidades de las RNN en la generación de texto que refleje el lenguaje natural y el estilo único de un personaje público conocido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\r\n",
    "\r\n",
    "En este proyecto, nos centraremos en la generación de texto en español utilizando Redes Neuronales Recurrentes (RNN) con Keras, inspirándonos en la técnica y el estilo de comunicación del conocido streamer Ibai Llanos.\r\n",
    "\r\n",
    "Este trabajo se basa en el tutorial de generación de texto de TensorFlow, disponible en [TensorFlow Tutorials](https://www.tensorflow.org/text/tutorials/text_generation?hl=es-419), y utiliza un dataset específico de Ibai Llanos proporcionado por [Aprende Machine Learning](https://www.aprendemachinelearning.com/). A diferencia del tutorial original, que se enfoca en la generación de texto en inglés, nuestro proyese adaptaráicas para trabajar con texto en español, reflejas y el estilo único de Ibai Llanos.\r\n",
    "\r\n",
    "Los objetivos y pasos clave de este proyecto incluyen:\r\n",
    "\r\n",
    "- Preprocesamiento y tokenización del texto en español utilizando herramientas adecuadas para mantener la integridad del lenguaje y estilo.\r\n",
    "- Construcción y entrenamiento de un modelo RNN con Keras, ajustado para capturar y replicar el estilo de Ibai Llanos en la generación de texto.\r\n",
    "- Implementación de técnicas de generaciónpara intentar que elermitan pueda al modelo producir respuestas coherentes y creativas en español, utilizando las características lingüísticas y de estilo del dataset.\r\n",
    "\r\n",
    "Nuestro enfoque permitirá explorar las capacidades de las RNN en la generación de texto en un idioma diferente al inglés y en capturar el estilo de una personalidad específica, ampliando así el alcance y la aplicabilidad de los modelos de generación de texto en el campo del procesamiento del lenguaje natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importar TensorFlow y otras bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lectura de ficheros de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "datos_ibai = \"transcripcionesIbai.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leer los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aavnuByVymwK",
    "outputId": "3a8a4f80-f3c1-4335-e79c-3dbc9e652b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud del texto: 2139034 caracteres\n"
     ]
    }
   ],
   "source": [
    "# Leer el archivo y luego decodificarlo.\n",
    "text = open(datos_ibai, 'rb').read().decode(encoding='utf-8')\n",
    "# La longitud del texto es el número de caracteres que contiene\n",
    "print(f'Longitud del texto: {len(text)} caracteres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Duhg9NrUymwO",
    "outputId": "30732c44-29f3-48a8-e086-11fe36435e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hola, buenas tardes.  por fin.  hola.  creo que estoy  sí, te escucho, pero a mí me está dando  ¿ me escuchas ? estoy muy nervioso, eh. \r\n",
      " perdóname, dios.  me acabo de poner mazo de nervioso, tío.  un segundo, ¿ eh ? vale.  ¿ qué tal ? ¿ todo bien \n"
     ]
    }
   ],
   "source": [
    "# Echar un vistazo a los primeros 250 caracteres del texto\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlCgQBRVymwR",
    "outputId": "80250569-ce6d-4bf2-a02d-1cbbf1c4f0fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 caracteres únicos\n"
     ]
    }
   ],
   "source": [
    "# Los caracteres únicos en el archivo\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} caracteres únicos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Procesar el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vectorizar el texto\n",
    "\n",
    "\n",
    "Antes del entrenamiento, debe convertir las cadenas en una representación numérica. <br>\n",
    "La capa `tf.keras.layers.StringLookup` puede convertir cada carácter en un ID numérico. Solo necesita que el texto se divida en tokens primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a86OoYtO01go",
    "outputId": "f054b9a6-8783-4ef2-c408-b0710844d721"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creamos la capa `tf.keras.layers.StringLookup` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6GMlCe3qzaL9"
   },
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLv5Q_2TC2pc",
    "outputId": "4fd28f2a-8bf2-4ca0-f474-ca7310e528ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[28, 29, 30, 31, 32, 33, 34], [51, 52, 53]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que el objetivo de este tutorial es generar texto, también será importante invertir esta representación y recuperar cadenas legibles por humanos a partir de ella. Para esto, puede usar `tf.keras.layers.StringLookup(..., invert=True)`.\n",
    "\n",
    "Nota: Aquí, en lugar de pasar el vocabulario original generado con `sorted(set(text))` use el método `get_vocabulary()` de la capa `tf.keras.layers.StringLookup` para que los tokens `[UNK]` se configuren de la misma manera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Wd2m3mqkDjRj"
   },
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta capa recupera los caracteres de los vectores de ID y los devuelve como un `tf.RaggedTensor` de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2GCh0ySD44s",
    "outputId": "39f89c4a-8503-4318-85d9-75cff9a03e8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede `tf.strings.reduce_join` para volver a unir los caracteres en cadenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxYI-PeltqKP",
    "outputId": "62a62ca2-0222-47c8-f738-db9bcb20e22f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "w5apvBDn9Ind"
   },
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La tarea de predicción\n",
    "\r\n",
    "Dado un carácter, o una secuencia de caracteres, ¿cuál es el próximo carácter más probable? Esta es la tarea para la que está entrenando al modelo. La entrada al modelo será una secuencia de caracteres, y usted entrena el modelo para predecir la salida: el siguiente carácter en cada paso de tiempo.\r\n",
    "\r\n",
    "Dado que los RNN mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres computados hasta este momento, ¿cuál es el siguiente carácter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Crear ejemplos de entrenamiento y objetivos\n",
    "\n",
    "A continuación, dividimos el texto en secuencias de ejemplo. Cada secuencia de entrada contendrá caracteres `seq_length` del texto.\n",
    "\n",
    "Para cada secuencia de entrada, los objetivos correspondientes contienen la misma longitud de texto, excepto que se desplaza un carácter a la derecha.\n",
    "\n",
    "Así que divide el texto en partes de `seq_length+1` . Por ejemplo, digamos que `seq_length` es 4 y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hola\" y la secuencia objetivo \"ola\".\n",
    "\n",
    "Para hacer esto, primero utiliza la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en un flujo de índices de carcteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMgNDAa0VEiM",
    "outputId": "d6b828a0-ac21-4665-92bf-17c74ca10f88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2139034,), dtype=int64, numpy=array([ 3, 35, 42, ..., 42, 14,  3])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yU4QQ-d_VEiM"
   },
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf6kDT-5VEiM",
    "outputId": "c19d3eba-0434-490a-b435-ce51a7a3234b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "h\n",
      "o\n",
      "l\n",
      "a\n",
      ",\n",
      " \n",
      "b\n",
      "u\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sp5p_95oVEiM"
   },
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRxbSQe5VEiM"
   },
   "source": [
    "A continuación vamos a procesar el `Dataset` de IDs de caracteres para formar secuencias de entrenamiento para el modelo.\n",
    "\n",
    "1. `sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)`: Esta línea agrupa los IDs en lotes, cada uno de tamaño `seq_length+1`. El argumento `drop_remainder=True` asegura que todos los lotes tengan exactamente este tamaño, descartando cualquier lote más pequeño al final del conjunto de datos.\n",
    "\n",
    "2. El bucle `for` con `sequences.take(1)` muestra un ejemplo de estas secuencias. Al imprimir `chars_from_ids(seq)`, estamos convirtiendo una secuencia de IDs nuevamente en caracteres para visualizar cómo se ve una secuencia de entrenamiento típica.\n",
    "\n",
    "Este paso es crucial para estructurar los datos de manera que el modelo de aprendizaje automático pueda aprender a predecir el siguiente carácter en una secuencia de un tamaño fijo, en este caso, `seq_length+1`. Esta estructuración es un paso preparatorio estándar en la creación de modelos de procesamiento de lenguaje natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTEdxbshVEiM",
    "outputId": "70c4579d-c960-43d0-f1bc-35b147b0f16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b' ' b'h' b'o' b'l' b'a' b',' b' ' b'b' b'u' b'e' b'n' b'a' b's' b' '\n",
      " b't' b'a' b'r' b'd' b'e' b's' b'.' b' ' b' ' b'p' b'o' b'r' b' ' b'f'\n",
      " b'i' b'n' b'.' b' ' b' ' b'h' b'o' b'l' b'a' b'.' b' ' b' ' b'c' b'r'\n",
      " b'e' b'o' b' ' b'q' b'u' b'e' b' ' b'e' b's' b't' b'o' b'y' b' ' b' '\n",
      " b's' b'\\xc3\\xad' b',' b' ' b't' b'e' b' ' b'e' b's' b'c' b'u' b'c' b'h'\n",
      " b'o' b',' b' ' b'p' b'e' b'r' b'o' b' ' b'a' b' ' b'm' b'\\xc3\\xad' b' '\n",
      " b'm' b'e' b' ' b'e' b's' b't' b'\\xc3\\xa1' b' ' b'd' b'a' b'n' b'd' b'o'\n",
      " b' ' b' ' b'\\xc2\\xbf' b' ' b'm' b'e'], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9DGog5DVEiP"
   },
   "source": [
    "Vamos a iterar sobre las primeras cinco secuencias del `Dataset` de TensorFlow y visualizándolas:\n",
    "\n",
    "1. El bucle `for` con `sequences.take(5)` selecciona las primeras cinco secuencias del conjunto de datos.\n",
    "\n",
    "2. `print(text_from_ids(seq).numpy())`: Dentro del bucle, convertimos cada secuencia de IDs numéricos de vuelta a texto usando la función `text_from_ids` y luego lo convertimos a un arreglo NumPy para imprimirlo.\n",
    "\n",
    "Este proceso nos permite ver cómo se han transformado las secuencias de texto originales en secuencias de IDs y luego de vuelta a texto. Es una forma de verificar que la conversión de datos se ha realizado correctamente y de entender mejor cómo se están preparando los datos para el entrenamiento del modelo. Esta visualización es una práctica común para asegurarse de que los datos se procesan como se espera antes de proceder con el entrenamiento del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_S0Xu2ptVEiQ",
    "outputId": "731c6fc6-bafa-4192-ff28-ca69a900ab20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b' hola, buenas tardes.  por fin.  hola.  creo que estoy  s\\xc3\\xad, te escucho, pero a m\\xc3\\xad me est\\xc3\\xa1 dando  \\xc2\\xbf me'\n",
      "b' escuchas ? estoy muy nervioso, eh. \\r\\n perd\\xc3\\xb3name, dios.  me acabo de poner mazo de nervioso, t\\xc3\\xado.  un'\n",
      "b' segundo, \\xc2\\xbf eh ? vale.  \\xc2\\xbf qu\\xc3\\xa9 tal ? \\xc2\\xbf todo bien ? todo bien. \\r\\n aqu\\xc3\\xad.  viendo que equipo va a coger, '\n",
      "b'no s\\xc3\\xa9 qu\\xc3\\xa9 equipo coge.  no s\\xc3\\xa9 si estoy contigo, conectado, no s\\xc3\\xa9 qu\\xc3\\xa9 es lo que  s\\xc3\\xad, s\\xc3\\xad, est\\xc3\\xa1s conmigo'\n",
      "b'.  el otro de la partida soy yo. \\r\\n el otro de la partida soy yo, s\\xc3\\xad.  ok, ya, ya.  bueno, como lo pr'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm7c5HFaVEiQ"
   },
   "source": [
    "Para el entrenamiento, necesitará un conjunto de datos de (input, label) pares. Donde input y label son secuencias. En cada paso de tiempo, la entrada es el carácter actual y la etiqueta es el siguiente carácter.\r\n",
    "\r\n",
    "Aquí hay una función que toma una secuencia como entrada, la duplica y la cambia para alinear la entrada y la etiqueta para cada paso de tiempo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "acFLQWlOVEiQ"
   },
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q55fYYoSVEiQ",
    "outputId": "c7588c9f-3b99-4213-c3e9-e9888b98168b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "CzKE__b-VEiQ"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lJ3TMeGHVEiQ",
    "outputId": "0f6da4bf-d30e-43d7-b062-35cc480d9c21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b' hola, buenas tardes.  por fin.  hola.  creo que estoy  s\\xc3\\xad, te escucho, pero a m\\xc3\\xad me est\\xc3\\xa1 dando  \\xc2\\xbf m'\n",
      "Target: b'hola, buenas tardes.  por fin.  hola.  creo que estoy  s\\xc3\\xad, te escucho, pero a m\\xc3\\xad me est\\xc3\\xa1 dando  \\xc2\\xbf me'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Crear lotes de entrenamiento\n",
    "\n",
    "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, debe mezclar los datos y empaquetarlos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nqULnXblVEiQ",
    "outputId": "31bd9205-e78c-422a-99b8-19f03ffc54dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "#### 3. Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAdKY0REVEiQ"
   },
   "source": [
    "Los siguientes fragmentos de código define una clase de modelo personalizado utilizando TensorFlow Keras para un modelo de generación de texto:\n",
    "\n",
    "1. Parámetros del Modelo:\n",
    "   - `vocab_size = len(ids_from_chars.get_vocabulary())`: Define el tamaño del vocabulario, que es el número de caracteres únicos en el texto.\n",
    "   - `embedding_dim = 256`: Establece la dimensión de incrustación (embedding) en 256. Las incrustaciones son representaciones densas de bajo dimensionalidad de cada carácter.\n",
    "   - `rnn_units = 1024`: Especifica el número de unidades en la capa de Red Neuronal Recurrente (RNN).\n",
    "\n",
    "2. Clase `MyModel`:\n",
    "   - La clase `MyModel` hereda de `tf.keras.Model` y define un modelo de red neuronal.\n",
    "   - La capa `Embedding` transforma los índices de caracteres en vectores densos de tamaño `embedding_dim`.\n",
    "   - La capa `GRU` (una forma de RNN) procesa secuencias de vectores de incrustación y mantiene un estado interno que captura la información de los caracteres anteriores.\n",
    "   - La capa `Dense` convierte la salida de la RNN en probabilidades para cada carácter en el vocabulario.\n",
    "   - La función `call` define cómo pasa la entrada a través de estas capas. Si `return_state` es verdadero, devuelve tanto la salida como el estado interno, útil para generación de texto.\n",
    "\n",
    "3. Instanciación del Modelo:\n",
    "   - `model = MyModel(vocab_size, embedding_dim, rnn_units)`: Crea una instancia del modelo con los parámetros de vocabulario, dimensión de incrustación y unidades RNN especificados.\n",
    "\n",
    "Este modelo es adecuado para tareas de generación de texto, donde el objetivo es predecir el siguiente carácter en una secuencia basándose en los caracteres anteriores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "m8gPwEjRzf-Z"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Y-vBEU0JVEiQ"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HWuFMIRZVEiQ"
   },
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "#### 4. Probando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sa79AQOVEiR"
   },
   "source": [
    "El siguiente código demuestra cómo realizar una predicción con un lote de datos utilizando el modelo definido previamente:\n",
    "\n",
    "1. El bucle `for` recorre un solo lote del conjunto de datos `dataset`. Este lote contiene pares de secuencias de entrada (`input_example_batch`) y sus correspondientes secuencias objetivo (`target_example_batch`).\n",
    "\n",
    "2. `example_batch_predictions = model(input_example_batch)`: Aquí, el modelo realiza predicciones en el lote de entradas. El modelo devuelve la salida para cada secuencia de entrada en el lote.\n",
    "\n",
    "3. `print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")`: Imprime la forma de las predicciones. La salida tiene tres dimensiones: tamaño del lote (`batch_size`), longitud de la secuencia (`sequence_length`) y tamaño del vocabulario (`vocab_size`). Esta forma indica que, para cada carácter en cada secuencia del lote, el modelo ha generado un vector de probabilidad sobre el vocabulario entero, representando la probabilidad de cada carácter de ser el siguiente en la secuencia.\n",
    "\n",
    "Este paso es crucial para verificar la estructura de salida del modelo y asegurarse de que está generando predicciones en la forma esperada, lo cual es fundamental para proceder con el entrenamiento y la evaluación del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sy-lEX62VEiR",
    "outputId": "b217e2be-e61d-4713-c20b-d1e9be9afc71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 115) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w--hbB0WVEiR",
    "outputId": "a6fb09c0-4e14-4e72-aea0-b33166d1ecb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  29440     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  117875    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4085619 (15.59 MB)\n",
      "Trainable params: 4085619 (15.59 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUzUWAP3VEiR"
   },
   "source": [
    "A continuación vamos a seleccionar caracteres al azar de las predicciones del modelo para una secuencia de ejemplo:\n",
    "\n",
    "1. `sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)`: Utiliza la función `tf.random.categorical` para muestrear índices de caracteres basándose en las distribuciones de probabilidad proporcionadas por `example_batch_predictions[0]`, que representa las predicciones del modelo para el primer elemento en el lote. El parámetro `num_samples=1` indica que se seleccionará un carácter por cada paso de tiempo en la secuencia.\n",
    "\n",
    "2. `sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()`: La función `tf.squeeze` elimina las dimensiones de tamaño 1 del tensor `sampled_indices`, simplificando su estructura. Luego, se convierte el tensor resultante en un arreglo NumPy.\n",
    "\n",
    "Estos pasos son parte del proceso de generar texto con el modelo: basándose en las predicciones del modelo, seleccionamos al azar el siguiente carácter en cada paso de tiempo, lo que nos permite construir una secuencia de caracteres generada por el modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "K7FWSB3RVEiR"
   },
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY8hAoIvVEiR"
   },
   "source": [
    "El siguiente código nos dará, en cada paso de tiempo, una predicción del siguiente índice de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7paRDXeVEiR",
    "outputId": "aa5384db-dfab-48dd-f4fc-b7f7ea6e4824"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 23,  78, 109,  93,  63,  66,  23,  88,  78,   2,  24,  43,  69,\n",
       "       104, 114,  94,  51,  81,  59,  76,   1,  91,   5,  93,  35,  99,\n",
       "        22,  41,  54,  86,   0,  50,  95,  18,  72, 111,  32,  95,  71,\n",
       "        26,  84,  28, 108, 112,  13,  84,  68,  25,  74, 101,  13,  42,\n",
       "        13,  97,  30,  51,   5,  22,   3,  62,  83, 101,  97,  67,  15,\n",
       "        55,  34,  95,  31,  76,   4,  47,  10,  84,  95,  16,  68,  82,\n",
       "        74,  11,  69, 104,  44,  32,  96,  66,  68,  71, 104,  35,  43,\n",
       "       104,  92,  39,  70, 110,  42,  18,  68,  88])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMfQPeeiVEiR"
   },
   "source": [
    "Decodificamos estos para ver el texto predicho por este modelo no entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5_9XvxdWVEiR",
    "outputId": "575452da-cc34-48e3-b467-b73b37ed6b9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:\n",
      " b' \\r\\n o sea, ir por una nota muy grave y de la nada pasar a una muy aguda, cosa que hice en colocao va'\n",
      "\n",
      "Predicciones del Siguiente Carácter:\n",
      " b'8\\xd0\\xb2\\xec\\x98\\xa4\\xe5\\x80\\x91\\xc3\\xa5\\xc3\\xaa8\\xe3\\x83\\x88\\xd0\\xb2\\r9p\\xc3\\xb2\\xeb\\xa8\\xb9\\xf0\\x9f\\x8e\\xb5\\xe6\\x83\\xb4x\\xd0\\xb6\\xc2\\xbf\\xcf\\x87\\n\\xe3\\x83\\xbc\"\\xe5\\x80\\x91h\\xea\\xb0\\x997n|\\xd1\\x87[UNK]w\\xe6\\xae\\x983\\xc3\\xb6\\xec\\xb6\\xa4e\\xe6\\xae\\x98\\xc3\\xb4>\\xd1\\x80a\\xec\\x96\\xb4\\xed\\x99\\x8d-\\xd1\\x80\\xc3\\xb1<\\xc3\\xbc\\xeb\\x86\\x93-o-\\xe8\\xb4\\x8fcx\"7 \\xc3\\xa4\\xd0\\xbf\\xeb\\x86\\x93\\xe8\\xb4\\x8f\\xc3\\xad0\\xc2\\xa1g\\xe6\\xae\\x98d\\xcf\\x87!t*\\xd1\\x80\\xe6\\xae\\x981\\xc3\\xb1\\xd0\\xb7\\xc3\\xbc+\\xc3\\xb2\\xeb\\xa8\\xb9qe\\xe7\\x97\\x85\\xc3\\xaa\\xc3\\xb1\\xc3\\xb4\\xeb\\xa8\\xb9hp\\xeb\\xa8\\xb9\\xe4\\xbb\\xacl\\xc3\\xb3\\xec\\x9b\\xa0o3\\xc3\\xb1\\xe3\\x83\\x88'\n"
     ]
    }
   ],
   "source": [
    "print(\"Entrada:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Predicciones del Siguiente Carácter:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en los resultados proporcionados, aquí tienes un comentario adaptado:\n",
    "En esta parte del notebook, estamos observando la decodificación y visualización de la salida de un modelo de lenguaje que aún no ha sido entrenado. Esta práctica es crucial para entender cómo el modelo realiza predicciones antes de ser sometido a un proceso de aprendizaje. Analicemos en detalle lo que está sucediendo:\n",
    "\n",
    "1. **Entrada Decodificada**:\n",
    "   - La entrada (`Entrada`) muestra una parte de un texto que parece ser una conversación o un comentario sobre música, evidenciado por la referencia a notas musicales graves y agudas.\n",
    "   - La cadena de entrada es: `\" o sea, ir por una nota muy grave y de la nada pasar a una muy aguda, cosa que hice en colocao va\"`.\n",
    "\n",
    "2. **Predicciones de Siguiente Carácter**:\n",
    "   - Las `Predicciones del Siguiente Carácter` son las estimaciones del modelo sobre qué caracteres podrían seguir a la entrada proporcionada.\n",
    "   - Dado que el modelo aún no está entrenado, estas predicciones son fundamentalmente aleatorias, resultando en una secuencia sin sentido compuesta por una mezcla de caracteres, incluyendo letras, números, símbolos y caracteres especiales.\n",
    "\n",
    "**Análisis**:\n",
    "- Esta salida ilustra el comportamiento de un modelo de lenguaje no entrenado, donde realiza predicciones basadas en la aleatoriedad en lugar de en el conocimiento del lenguaje. La falta de coherencia y sentido en las predicciones refleja la ausencia de aprendizaje previo del modelo.\n",
    "- Esta etapa es ideal para comprender el estado inicial del modelo, que aún no ha adquirido la capacidad de entender o generar texto con estructura gramatical o sintáctica coherente.\n",
    "\n",
    "**Conclusión**:\n",
    "- Observar las predicciones de un modelo de lenguaje no entrenado es útil para establecer una base antes de su entrenamiento. Permite apreciar el impacto y la importancia del entrenamiento en la evolución del modelo hacia la generación de texto coherente y contextualmente adecuado.\n",
    "- Este proceso destaca cómo el entrenamiento transforma un modelo de generar respuestas aleatorias a producir texto que es gramaticalmente correcto y contextualmente relevante, demostrando la importancia del entrenamiento en el desarrollo de modelos de lenguaje efectivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N70QuxJOVEiR"
   },
   "source": [
    "#### 5. Entrenando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaIMdFQKVEiS"
   },
   "source": [
    "##### Adjuntamos un optimizador y una función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwls0dS2VEiS"
   },
   "source": [
    "Vamos a calcular la pérdida (loss) media para un lote de ejemplos, un paso crucial en el entrenamiento y la evaluación de modelos de aprendizaje automático:\n",
    "\n",
    "1. `loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)`: Define la función de pérdida como entropía cruzada categórica dispersa. El parámetro `from_logits=True` indica que la función debe aplicarse a las salidas del modelo que son logits (valores previos a la aplicación de una función de activación como softmax). Esta función de pérdida es adecuada para problemas de clasificación multiclase como la generación de texto, donde cada carácter generado se trata como una clase distinta.\n",
    "\n",
    "2. `example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)`: Calcula la pérdida media para un lote de ejemplos comparando las predicciones del modelo (`example_batch_predictions`) con las secuencias objetivo reales (`target_example_batch`). La función de pérdida evalúa qué tan bien las predicciones del modelo coinciden con los objetivos reales.\n",
    "\n",
    "3. Las líneas `print` muestran la forma de las predicciones y la pérdida media. La forma de las predicciones confirma que el modelo está generando salidas en el formato esperado (`batch_size`, `sequence_length`, `vocab_size`). La pérdida media proporciona una medida de cuán precisas son las predicciones del modelo: una pérdida baja indica predicciones más precisas.\n",
    "\n",
    "Calcular y monitorear la pérdida media es esencial para entender el rendimiento del modelo y para ajustar los parámetros del modelo o el proceso de entrenamiento con el fin de mejorar la precisión de las predicciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JtZ5AZ8XVEiS"
   },
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0wjJ7OiVEiS",
    "outputId": "d2df90cc-fcd5-4bab-ee07-12cce8599250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de la predicción:  (64, 100, 115)  # (tamaño del lote, longitud de la secuencia, tamaño del vocabulario)\n",
      "Pérdida media:         tf.Tensor(4.7424912, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Forma de la predicción: \", example_batch_predictions.shape, \" # (tamaño del lote, longitud de la secuencia, tamaño del vocabulario)\")\n",
    "print(\"Pérdida media:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr8LtkLlVEiS"
   },
   "source": [
    "En la siguiente línea de código se calcula y se muestra el valor exponencial de la pérdida media calculada previamente para el lote de ejemplos:\n",
    "\n",
    "- `tf.exp(example_batch_mean_loss).numpy()`: Aplica la función exponencial (`tf.exp`) a la pérdida media (`example_batch_mean_loss`) y convierte el resultado a un arreglo NumPy. Este cálculo transforma la pérdida media, que es una medida de error, en una métrica más interpretable. En el contexto del aprendizaje automático, especialmente en problemas de clasificación como la generación de texto, este valor puede proporcionar una indicación más intuitiva de la efectividad del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAfVL2e3VEiS",
    "outputId": "a87427dc-767f-4458-d6d1-6479debf7a19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.71965"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilamos el modelo, especificando el optimizador y la función de pérdida:\n",
    "\n",
    "- `model.compile(optimizer='adam', loss=loss)`: Configura el modelo para el entrenamiento. Se utiliza el optimizador 'adam', un algoritmo de optimización popular para entrenar redes neuronales, y se establece la función de pérdida definida previamente (`loss`). La compilación es un paso esencial antes de comenzar el entrenamiento, ya que establece cómo el modelo debe actualizarse y evaluarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Wug55EuJVEiS"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMF8TZzkVEiS"
   },
   "source": [
    "##### Configurar checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "bluukf7zVEiS"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QajTC-LAVEiS"
   },
   "source": [
    "##### Ejecutamos el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1bnUujGnVEiS"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWx0mf5zVEiS",
    "outputId": "d8ee7268-04c1-48b5-c928-db987c300230"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "330/330 [==============================] - 25s 62ms/step - loss: 2.2365\n",
      "Epoch 2/30\n",
      "330/330 [==============================] - 22s 61ms/step - loss: 1.5936\n",
      "Epoch 3/30\n",
      "330/330 [==============================] - 21s 60ms/step - loss: 1.3721\n",
      "Epoch 4/30\n",
      "330/330 [==============================] - 21s 60ms/step - loss: 1.2730\n",
      "Epoch 5/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 1.2138\n",
      "Epoch 6/30\n",
      "330/330 [==============================] - 22s 62ms/step - loss: 1.1694\n",
      "Epoch 7/30\n",
      "330/330 [==============================] - 22s 59ms/step - loss: 1.1316\n",
      "Epoch 8/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 1.0984\n",
      "Epoch 9/30\n",
      "330/330 [==============================] - 21s 60ms/step - loss: 1.0665\n",
      "Epoch 10/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 1.0346\n",
      "Epoch 11/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 1.0029\n",
      "Epoch 12/30\n",
      "330/330 [==============================] - 22s 59ms/step - loss: 0.9717\n",
      "Epoch 13/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 0.9405\n",
      "Epoch 14/30\n",
      "330/330 [==============================] - 21s 60ms/step - loss: 0.9097\n",
      "Epoch 15/30\n",
      "330/330 [==============================] - 22s 61ms/step - loss: 0.8810\n",
      "Epoch 16/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 0.8534\n",
      "Epoch 17/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 0.8282\n",
      "Epoch 18/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 0.8063\n",
      "Epoch 19/30\n",
      "330/330 [==============================] - 22s 61ms/step - loss: 0.7873\n",
      "Epoch 20/30\n",
      "330/330 [==============================] - 22s 61ms/step - loss: 0.7709\n",
      "Epoch 21/30\n",
      "330/330 [==============================] - 22s 58ms/step - loss: 0.7575\n",
      "Epoch 22/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 0.7451\n",
      "Epoch 23/30\n",
      "330/330 [==============================] - 23s 61ms/step - loss: 0.7373\n",
      "Epoch 24/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 0.7301\n",
      "Epoch 25/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 0.7253\n",
      "Epoch 26/30\n",
      "330/330 [==============================] - 23s 61ms/step - loss: 0.7196\n",
      "Epoch 27/30\n",
      "330/330 [==============================] - 21s 60ms/step - loss: 0.7181\n",
      "Epoch 28/30\n",
      "330/330 [==============================] - 22s 61ms/step - loss: 0.7190\n",
      "Epoch 29/30\n",
      "330/330 [==============================] - 22s 60ms/step - loss: 0.7169\n",
      "Epoch 30/30\n",
      "330/330 [==============================] - 21s 59ms/step - loss: 0.7189\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El proceso de entrenamiento de nuestro modelo RNN a lo largo de estas 30 épocas ha mostrado una notable eficiencia en comparación con la práctica anterior, donde se necesitaban al menos 300 épocas para lograr un texto coherente. Analicemos los resultados obtenidos y las posibles razones de esta mejora:\r\n",
    "\r\n",
    "- **Inicio del Entrenamiento**: Durante las primeras épocas, se observa una rápida disminución en la pérdida (`loss`), pasando de 2.2365 a 1.5936. Este descenso rápido es indicativo de que el modelo está captando las estructuras básicas del lenguaje de manera eficiente, posiblemente debido a la amplia variedad y riqueza del nuevo dataset.\r\n",
    "\r\n",
    "- **Progresión Continua de la Pérdida**: A medida que avanzan las épocas, la pérdida sigue disminuyendo, aunque a un ritmo más gradual. Esto sugiere que el modelo está afinando su aprendizaje, adaptándose a las complejidades y matices del texto. La pérdida se reduce de manera constante, pasando por valores como 1.2730, 1.0984 y 0.9717, lo que demuestra un proceso de aprendizaje sostenido.\r\n",
    "\r\n",
    "- **Estabilización hacia el Final del Entrenamiento**: En las últimas épocas, la pérdida se estabiliza alrededor de 0.7189, mostrando fluctuaciones menores. Esta estabilización indica que el modelo ha alcanzado un equilibrio en su aprendizaje, absorbiendo eficazmente la información del dataset sin sobreajustarse.\r\n",
    "\r\n",
    "**Comparación con la Práctica 2**:\r\n",
    "- Es notable que, en comparación con la práctica 2 donde se necesitaban al menos 300 épocas, en esta ocasión solo hemos requerido 30 épocas para obtener resultados coherentes. Esto se puede atribuir a la mayor extensión y diversidad del nuevo dataset utilizado, que ha proporcionado una base de datos más rica y variada para el entrenamiento.\r\n",
    "\r\n",
    "**Conclusión**:\r\n",
    "- La eficiencia del entrenamiento en este proyecto demuestra cómo un dataset extenso y diverso puede mejorar significativamente la capacidad de aprendizaje de un modelo de lenguaje. La disminución constante y la estabilización de la pérdida sugieren que el modelo ha logrado captar la esencia del lenguaje y estilo de Ibai Llanos, lo que se espera se refleje en la calidad del texto generado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "#### 6. Generación texto y evaluación de su calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "fz_fg7lgVEiS"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.8):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Crear una máscara para evitar que se genere \"[UNK]\".\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Poner un -inf en cada índice no deseado.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Ajustar la forma al vocabulario\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convertir cadenas de texto en IDs de tokens.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Ejecutar el modelo.\n",
    "    # La forma de predicted_logits es [lote, carácter, logits_del_siguiente_carácter]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Utilizar solo la última predicción.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Aplicar la máscara de predicción: evitar que se genere \"[UNK]\".\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Muestrear los logits de salida para generar IDs de tokens.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convertir de IDs de tokens a caracteres\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Devolver los caracteres y el estado del modelo.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "0H8KyjfUVEiS"
   },
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7NV-9_X-VEiT",
    "outputId": "a6e46a4a-630f-4e54-fe03-e4f1ef6ce88d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿ que no hay que hacerla ? pues ya te digo. \r\n",
      " ayer lo tengo yo también.  una setup privada siempre.  hace años ya.  ¿ qué es esto ? esto no es booque townetó, coño.  es que es como uno de los shows gorno.  se ha de bates el mejor. \r\n",
      " mira.  voy a decir dos cosas.  vences entendrás que estabas repartido como el mes mío, vamos a hacer un tema cuando lo hacía con su favorito sería una carencia de que hablaba es como yo mí me gustaba encontrara que se llevando covid.  y no lo pienso, no sé qué ha pasado, ¿ no ? ¿ quieres escuchar este año ? ¿ de quiénes se seguías ? que lo grabé una barbarsica para que geste nunca tengo el visa y el resto va a foca 300.  ya he visto el castel.  me he caído.  es cara esta, está lleno de coimbra.  la verdad es que no sé qué decía k, soy yo calla, hijo del granjero, te lo digo, como el de la bandera correcta de puerto rico, que al final es lo que hablamos y están ahí escuchando una cosa.  hemos hecho muchas cosas como son. \r\n",
      " es como un sebado bastante noble, la verdad.  no, es un punto aparcar en todo.  claro, tío.  eso es el chaleche número ganas de pasar.  es un 50  56, 55.  55, sí, con mis colecciones argentinos, la verde del hogar, de un millón.  pero eso sí que lo están haciendo un hijo de puta conmigo y tiene mucho mejor a grefg.  y dije, bueno  el cuarto lo entiendo. \r\n",
      " es el momento lo estoy cerca.  ya, pero tienes una idea que te cae mejor con el barrio y me dijo que él es tú.  no hay dinero no entendía. \r\n",
      " eh  ¿ quién es ganar, saxo gaming ? ¿ hasta una plática que vas a hacer mucho ? no me lo toques no, pero es cierto que  eso de nada. \r\n",
      " ¿ qué quieres ? ah, coño, que es el cambio ahora mismo es josepedre leros.  los de la fórmula 1 están todo el día antes. \r\n",
      " lleva una parte chill.  o sea, en el camp nou, va, va.  a ver.  vale, vale.  y voy a contestar, la rita.  ¡ay, dios mío! hasta venir otra mal.  buscableme por otra el mago tiene sus temperas.  galletas, pues hace diez esta operación de que estábamos con esas cosas que me habías dicho, pero que sí, coño. \r\n",
      " no sé dónde estoy.  ah, vale, fue  ¿ te acuerdas de una serie ? qué guapo está este coco.  se puede no vas a ser una barbe también, para parecer una tendencia parecida a matar. \r\n",
      " eh  yo no había barajo no me iba de vacación con esta camiseta.  ¿ a este hombre ? cabertura del barça. \r\n",
      " mister beast en el cove, este tío es increíble.  esto es caeró y tal.  yo creo que marcha de twitch.  ah, de la derecha.  la de la izquierda es la de la izquierda.  la de ven en la espera.  pero dependero mucho.  y un limito una asesina gruguada, el de la gente que hace el rato.  vale.  con las lesiados de verdad, es impresionando. \r\n",
      " ¿ sois bandesas están mano ? ¿ crees que esto es una cosa que se caga en la tele y está mi coche menos porque volvía a jugar contra elección española ? raso o a mi familia también que son de cambiarse o algo así.  el chico está esperando.  es decir, si se queda y esta cuenta que me he bloqueado y no tiene nada que ver como manalezco, me la recomiendo, perdió los precios se llevan catalunga del año, y singero que a lo mejor dice mucho a tu chat y en ese momento lo que vamos a tener todo y la universidad de con el celular f escepado, tú te respetas en bachita en el real madrid\r\n",
      " algunos sigan tiempo sentimiendo si quieres, te he visto en el combate anterior de una caja ¿ verdad ? ese es el granjero.  eso es un poco nervioso. \r\n",
      " muy bien que la gente crea miche con el culo.  a mí me cuesta meterme.  no me llames de medios.  confía en mi orden, no se repreía.  ¿ eso es la organiza ? no te ayudas no sé.  no sé ni yo sé cómo lo he hecho antes.  correcto.  es una contrasta y has grabado un cohete y tenemos a la final de la infancia.  y le quedaba, mi tamiño. \r\n",
      " la bueno de cerdibo se agradece por la polla.  te lo comentaba, ¿ viste ? de habilidad y me dice, ¿ o estás bien ? ¿ nunca tienes un club de fútbol, con todo esto ? a ver,<|ko|>.  no moleste en el sofá.  joder, vengo de puerto rico que es con todas las cosas que a lo mejor tengo  he visto mucha guitarra, tengo a reven y el barça no tiene para algo. \r\n",
      " pero bueno, ya lo hice.  no, cómo que me lo esperaba, ¿ eh ? pero vos de aquí tengo mi vida.  el taxque es marco.  un poquito más, no sabes ni la orgón.  decías «méxtcones, eh.  cuéntame tenemos el sonido de que hoy eh egol martínez se valoramos en tiktok y tú haces nada y me llevas muy bien con él. \r\n",
      " o sea, esto es un setup rutando bueno, vamos a empezar a verlo, es un punto que me la recomendó mucho de esforzar más te decían que el jueves ya con culé a cuco, nunca lo he hecho.  ah, tendría o sea, ¿ qué queréis que te digo ? creo que ha dado durante tres semanas es gol.  yo solo genera ti el récord rompí. \r\n",
      " el primer vídeo de ricardo, es un tema que ahí ya que solo te digo que me pasó un poco la televisión con una máscara y todo lo que hablábamos o con un periodista acá, así que queráis.  gracias a vivir del chat.  la gente empezó a caberlo con todo.  tienes que pasar a mi estam.  es un 5 cho de terronos, que tenemos tres años aquí, que estoy un poquito de alegría al hotel y va a decir un algo afí.  también lo tiene modo perdón. \r\n",
      " ha cogido como unborp martes y vivo en mis últimas básicamente que lo quiere meter.  ¿ cómo se hacía ? pues esto se ha jugado con él.  escuchamos para el kun agüero que es el chat, se nota que es una barbaridad.  a ver un segundo en españa. \r\n",
      " y bueno, yo qué sé.  me decís que es os dicen que no sé ni comparte la canción de españa.  no sé si había algo que decirlo.  lo que no me gusta nada vi que el silión a ver si el bat estí yo te abro un tema muy indicado para tendrá el brazo de tres deckert perros a máximo años y me has salido ya de la historia del mundo del sevilla, y espero que se hizo viral.  un tema que además hablando de un tiempo en chocas.  vale.  pensaba que había sido como un lomo sin quedar con ellos.  mucho lo veía y digo, ¿ te importa ? déjame tener una imagen de abusar a participando y tenemos un saludo a auron.  y esto tú ya tienes mi voz ahí, ¿ no ? no, carches dice que a cualquier barcelona no te veo ya, ¿ no ? es decir, tú viste esto sí que es cierto que tiene que ir a anunciar.  pregúntase la cámara que no te lo apetece.  que son de cambios.  mañana se va a revisar la copa del rey y llegar a ser gran carrera y se lo lleva a crear también.  wurk, te he robado un ¿ qué pedirías tú por la derecha, pero sí cuento más talentosa, pero mucho más el streaming de todos.  ¿ sí ? dj mario es un tumbarito muy bombitan.  pero aún así que si hacías un todo eso.  a mí me lo estoy logo porque jugaba, que vengan a cantar el mismo con el logo, tío.  no me fui de vacaciones.  estaba rompiendo récord y has venido a carchez, ¿ serías un profesional, y has sorprendido ? no, no me lo he escuchado nunca.  ¿ qué has hecho ? karchez ¿ qué pasa ? yo dije, no, de semsan la danza de ver si te pienso que se haya asquidado.  me flexentable. \r\n",
      " antes de la champions.  axel, ¿ has visto a auron ? me llamo de otras cosas.  ahí está.  el chabón lo tienes hostias de cualquier salud. \r\n",
      " ¿ de al azarmar en messi.  ¿ como me he hecho ? me he quedado una vez cómo es la que más te gusta.  hay un transallo, pero inventó bastante historia, que eso es una cosa que me ha hablado de hotel, en lo que hable, a casa y me dirá que lo otro y yo te veo a un tío con una superición de chister, a este sentillo lo no.  si me dices usted superimpacianos, señores, no te tires a nivel también.  ya como a muchísimo de ser anche! ayer  sí, sí, sí.  los partidos de shots. \r\n",
      " lo que haya sido aquí y se la suda.  a lo mejor te va a llevar siempre.  en españa ha hecho un tono.  sabes que ese casos que siempre está acostumbrado a un niño de cuarto nexicamente sentido.  ¿ crees que es una cosa del bulo0 ? es un 50  ozunaban en depresión no se había ido. \r\n",
      " pero como veis, lo tiene todo muy cantando, tío.  sí, sí.  ¿ me lo recuperas ? sí.  es un perro y un coco, dolli. \r\n",
      " hola, hermano, tío, no sabe nadie que vería, pero hacía un interpetido jugando al equipo, la verdad.  así que no me quedo que hacer.  o sea, que de temas muchas veces seguimos con la pelota y el segundo setup o que cuando nos metemos en la mierda, una teradía pendejo de haber vivido el vídeo, pero como algo menos español ¿ vale ? creo que han pasado muchos de acá, con ganas de hacer, ¿ te has sentido vosotros y lo vemos en el tema ? no tiene ningún sentido.  vengo en mi vida ya.  entonces, hacemos una leyenda en cuenta que la gente que está con ellos no se lo quita ander.  ¿ y monister era o con ciro no ? este giunety es un en cavor. \r\n",
      " ¿ prisiones durante española ? ah, vale, ahí está.  yo pensaba que eres tú, ¿ no ? por algo de cordo.  porque yo no me lo creo.  ¿ te acuerdas de la portuga tú el edad ? no he visto el detelliendo con la pausa.  no nos metieron en directo se hacía bien de hacer cosas arriesgadas, es que mi viejo me acabé en todo lo que tenemos.  al juego, no se me ve mi voz edante de ser un bebé te pusiste en una rendimiente. \r\n",
      " ah, pero estás todo lo que estamos hablando de un primer consejo a todos los equipos que digo es que messi se va en el colegio, pues la verdad es esa mierda.  un día te he dicho nada ok, viste, auron, qué sé yo, estás en mira qué tira, te he visto en una concha de tu universidad también de nuestra país's con la gente de la hostia ¿ qué opinas de eso ? con la polla de competicino. \r\n",
      " hijo puta, ¿ cómo se llama ? alberto.  bien, ale, parece un favor.  ¿ cuál es el lo entiendo ? sí, claro. \r\n",
      " correcto.  es de locos.  el post es de marcelo en tu setup, carrillo.  ahora, ya, a ver, es un poco como que la puedo acabar mi tú.  el otro día hablé con él desde casa tan conocido.  no hay tiempo en casa.  esta carrera he comentado antes ¿ no ? ¿ quién es demasiado más ? ¿ triste es este 2620 ? ¿ me van a montar un mes tacto kara jugando a la mano del codeza, que me quedé juntos.  con este minions está aquí con españa ? es como para mi ladicame, pero luego tienes ahí seguido, lo ha dicho  no sé muy bien.  que el viejo no es lo mismo q \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 30.03226637840271\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['¿ que no hay'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(10000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este texto generado por nuestro modelo RNN, entrenado para imitar el estilo comunicativo de Ibai Llanos, refleja una mezcla de coherencia y espontaneidad característica del influencer. A continuación, destaco algunos aspectos importantes del resultado:\r\n",
    "\r\n",
    "- **Estilo Informal y Conversacional**: El texto muestra un estilo informal y conversacional, con frases típicas de Ibai como \"¿que no hay que hacerla? pues ya te digo\" o \"ayer lo tengo yo también\". Esto sugiere que el modelo ha captado con éxito el tono coloquial y directo de Ibai.\r\n",
    "\r\n",
    "- **Elementos de Cultura Popular y Referencias Personales**: Se observan numerosas referencias a temas de actualidad, cultura popular y experiencias personales, como \"setup privada\", \"shows gorno\", \"covid\", o \"el de la bandera correcta de Puerto Rico\". Esto indica que el modelo ha aprendido a incorporar elementos que reflejan los intereses y el estilo de vida de Ibai.\r\n",
    "\r\n",
    "- **Fluidez y Repeticiones**: Aunque hay segmentos del texto que fluyen de manera coherente, también hay repeticiones y frases fragmentadas, como \"es cara esta, está lleno de Coimbra\" o \"el chico está esperando\". Estas incoherencias reflejan las limitaciones del modelo para mantener una narrativa completamente lógica y coherente.\r\n",
    "\r\n",
    "- **Comparación con Prácticas Anteriores**: En comparación con la práctica anterior, donde se necesitaban al menos 300 épocas para lograr coherencia, este modelo ha requerido solo 30 épocas para producir resultados significativos. Esto se debe probablemente a la mayor diversidad y extensión del dataset actual.\r\n",
    "\r\n",
    "**Conclusión**:\r\n",
    "- Los resultados muestran un modelo que ha aprendido a capturar varios aspectos del estilo y lenguaje de Ibai Llanos, aunque aún enfrenta desafíos en términos de coherencia y consistencia narrativa. La reducción del número de épocas necesarias, en comparación con la práctica anterior, destaca la importancia de un dataset amplio y variado para el entrenamiento eficiente de modelos de lenguaje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbFydchtVEiT"
   },
   "source": [
    "#### 8. Salvamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b_SNHG_gVEiT",
    "outputId": "dbfaa6a1-7ade-44ad-b539-1015a9237367"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7d3b6012ee30>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPA2eRuAVEiT",
    "outputId": "a822b1f5-a628-492f-da4a-3460b63d68a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿ te acuerdas ? correcto.  la escuchéis.  venga, va, que me pongo más europela de betos y con cristiano tenía la \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['¿ te acuerdas'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "d53d9c447d15846ca7228ba81a89f63b35afa7d922a1ed0608df97a83621769d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
