{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traducción de español a inglés con un Transformer secuencia a secuencia\n",
    "\n",
    "**Autores:** Alina Rojas y Mar Iborra<br>\n",
    "**Adaptado de:** [Ejemplo de Keras - Neural Machine Translation with Transformer](https://keras.io/examples/nlp/neural_machine_translation_with_transformer/)<br>\n",
    "**Basado en el trabajo original de:** [fchollet](https://twitter.com/fchollet)<br>\n",
    "**Fecha de creación de la adaptación:** 2023/12/31<br>\n",
    "**Última modificación:** 2024/01/12<br>\n",
    "**Descripción:** Este proyecto consiste en la implementación de un Transformer secuencia a secuencia para realizar tareas de traducción automática de español a inglés. Se ha adaptado del ejemplo de Keras, que fue diseñado originalmente para la traducción de inglés a español. Las adaptaciones incluyen ajustes en la preparación de los datos, la vectorización y la estructura del modelo para adecuarse a la traducción de español a inglés, basándose en las técnicas y el código proporcionados por F. Chollet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este proyecto, desarrollaremos un modelo Transformer secuencia a secuencia, el cual será entrenado para tareas de traducción automática de español a inglés.\n",
    "\n",
    "Este proyecto implica:\n",
    "\n",
    "- Vectorización de texto utilizando la capa `TextVectorization` de Keras.\n",
    "- Implementación de una capa `TransformerEncoder`, una capa `TransformerDecoder` y una capa `PositionalEmbedding`.\n",
    "- Preparación de datos para entrenar un modelo secuencia a secuencia.\n",
    "- Uso del modelo entrenado para generar traducciones de oraciones de entrada nunca antes vistas (inferencia secuencia a secuencia).\n",
    "\n",
    "El código utilizado se baso y adapto del libro [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition) (capítulo 11: Deep learning for text).\n",
    "\n",
    "\n",
    "\n",
    "### ¿Qué es un modelo Transformer secuencia a secuencia?\n",
    "\n",
    "Un modelo Transformer secuencia a secuencia es una arquitectura avanzada de aprendizaje profundo utilizada principalmente para tareas de traducción automática. Consiste en dos componentes clave: el codificador (encoder) y el decodificador (decoder). El codificador procesa la oración de entrada, mientras que el decodificador genera la traducción correspondiente. Este modelo es altamente eficaz para manejar secuencias de texto, gracias a su habilidad para captar dependencias a larga distancia y su capacidad para el procesamiento paralelo de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPOWReYZOf5d"
   },
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I5HleIZeOf5e"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cambios respecto al código original:\n",
    "\n",
    "En el código original, se hacía uso de `from keras import ops`, pero en nuestra adaptación, eliminamos esta importación. El cambio se debió a que `ops`, un submódulo de operaciones de bajo nivel, no es parte del módulo `keras` estándar, sino de TensorFlow (`tensorflow`). Por lo tanto, en lugar de utilizar `keras.ops`, optamos por usar directamente TensorFlow (`import tensorflow as tf`), que proporciona todas las funciones y operaciones necesarias.\r\n",
    "\r\n",
    "Este ajuste garantiza que todas las operaciones de TensorFlow se manejen de manera eficiente y compatible, evitando problemas de importación y compatibilidad que podrían surgir al intentar acceder a operaciones específicas de TensorFlow a través de Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descargando los Datos\n",
    "\n",
    "Trabajaremos con un conjunto de datos de traducción de inglés a español proporcionado por [Anki](https://www.manythings.org/anki/). Aunque el conjunto de datos original está diseñado para la traducción de inglés a español, adaptaremos el uso de estos datos para realizar traducciones en la dirección opuesta, de español a inglés. Descarguemos el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eJ-uQp8WOf5g",
    "outputId": "2ba4332e-3886-48a2-e44b-93eb3c198524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "2638744/2638744 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "text_file = keras.utils.get_file(\n",
    "    fname=\"spa-eng.zip\",\n",
    "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
    "    extract=True,\n",
    ")\n",
    "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este paso garantiza que tenemos acceso a un conjunto de datos adecuado que, aunque originalmente está enfocado en la traducción de inglés a español, será adaptado en nuestro código para soportar traducciones de español a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de los Datos\n",
    "\n",
    "Cada línea del conjunto de datos contiene una oración en inglés y su correspondiente traducción en español. En el enfoque original, la oración en inglés es la *secuencia de origen* y la española es la *secuencia objetivo*. Sin embargo, para nuestro proyecto, invertiremos este enfoque: la oración en español será nuestra secuencia de origen y la oración en inglés será la secuencia objetivo. Prependemos el token `\"[start]\"` y añadimos el token `\"[end]\"` a la oración en inglés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JL2tNYSxOf5i"
   },
   "outputs": [],
   "source": [
    "with open(text_file, encoding='utf-8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    eng = \"[start] \" + eng + \" [end]\"\n",
    "    text_pairs.append((spa, eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cambio en la preparación de los datos es crucial para adaptar el conjunto de datos a nuestro objetivo de traducción de español a inglés. Al modificar las secuencias objetivo, aseguramos que el modelo aprenda a traducir del español al inglés, lo cual es la finalidad principal de nuestro proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos ejemplos de cómo lucen nuestros pares de oraciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YkbdQqmOOf5k",
    "outputId": "1afb7aae-c8da-45b3-e6ee-9eb92eb312fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Los demás no le hicieron caso a su advertencia.', '[start] The others paid no attention to her warning. [end]')\n",
      "('Tom camina despacio.', '[start] Tom walks slowly. [end]')\n",
      "('Ella es muy amable con nosotras.', '[start] She is very kind to us. [end]')\n",
      "('A veces no le entiendo.', \"[start] I don't understand him sometimes. [end]\")\n",
      "('Tom compró un par de zapatos baratos, pero no le duraron mucho tiempo.', \"[start] Tom bought a pair of cheap shoes, but they didn't last very long. [end]\")\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cada par, la primera oración está en español y actúa como nuestra secuencia de entrada (origen), y la segunda oración está en inglés con los tokens `\"[start]\"` y `\"[end]\"` añadidos, sirviendo como nuestra secuencia objetivo. Estos ejemplos muestran cómo estamos preparando los datos para entrenar nuestro modelo de traducción de español a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, dividiremos los pares de oraciones en un conjunto de entrenamiento, un conjunto de validación y un conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KW7Qn6hbOf5l",
    "outputId": "d9b04ae1-3ea8-4a0e-aaa5-c567421b53fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118964 total pairs\n",
      "83276 training pairs\n",
      "17844 validation pairs\n",
      "17844 test pairs\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta división nos permite entrenar nuestro modelo en una variedad de oraciones, validar su rendimiento en un conjunto separado y, finalmente, evaluar su capacidad de generalización con oraciones que no ha visto antes en el conjunto de prueba. Este enfoque es esencial para desarrollar un modelo robusto y eficaz de traducción de español a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorización de los Datos Textuales\n",
    "\n",
    "Utilizaremos dos instancias de la capa `TextVectorization` para vectorizar los datos textuales (una para el español y otra para el inglés), es decir, para convertir las cadenas originales en secuencias de enteros donde cada entero representa el índice de una palabra en un vocabulario.\n",
    "\n",
    "La capa para el inglés usará la estandarización y el esquema de división predeterminados de `TextVectorization` (eliminar caracteres de puntuación y dividir en espacios en blanco), mientras que la capa para el español usará una estandarización personalizada, en la que añadiremos el carácter `\"¿\"` al conjunto de caracteres de puntuación que se eliminarán.\n",
    "\n",
    "Nota del trabajo original: en un modelo de traducción automática de grado productivo, no recomendaría eliminar los caracteres de puntuación en ninguno de los idiomas. En su lugar, recomendaría convertir cada carácter de puntuación en su propio token, lo que se podría lograr proporcionando una función `split` personalizada a la capa `TextVectorization`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "KKsFuXU6Of5m"
   },
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf_strings.lower(input_string)\n",
    "    return tf_strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    ")\n",
    "train_spa_texts = [pair[0] for pair in train_pairs]\n",
    "train_eng_texts = [pair[1] for pair in train_pairs]\n",
    "spa_vectorization.adapt(train_spa_texts)\n",
    "eng_vectorization.adapt(train_eng_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los cambios realizados con respecto al código original reflejan la adaptación para la traducción de español a inglés. La personalización en la vectorización del español, añadiendo el carácter `\"¿\"` a los caracteres de puntuación que se eliminan, es un ajuste específico para el idioma español, lo que mejora la calidad de la vectorización en este contexto particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, formatearemos nuestros conjuntos de datos.\n",
    "\n",
    "En cada paso de entrenamiento, el modelo intentará predecir las palabras objetivo N+1 (y siguientes) utilizando la oración fuente y las palabras objetivo de 0 a N.\n",
    "\n",
    "Por lo tanto, el conjunto de datos de entrenamiento generará una tupla `(inputs, targets)`, donde:\n",
    "\n",
    "- `inputs` es un diccionario con las claves `encoder_inputs` y `decoder_inputs`. `encoder_inputs` es la oración fuente vectorizada y `decoder_inputs` es la oración objetivo hasta el momento, es decir, las palabras de 0 a N utilizadas para predecir la palabra N+1 (y siguientes) en la oración objetivo.\n",
    "- `target` es la oración objetivo desplazada un paso: proporciona las siguientes palabras en la oración objetivo, lo que el modelo intentará predecir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ByubWjbwOf5m"
   },
   "outputs": [],
   "source": [
    "def format_dataset(spa, eng):\n",
    "    spa = spa_vectorization(spa)\n",
    "    eng = eng_vectorization(eng)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": spa,\n",
    "            \"decoder_inputs\": eng[:, :-1],\n",
    "        },\n",
    "        eng[:, 1:],\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    spa_texts, eng_texts = zip(*pairs)\n",
    "    spa_texts = list(spa_texts)\n",
    "    eng_texts = list(eng_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((spa_texts, eng_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.cache().shuffle(2048).prefetch(16)\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparación con el código original, este fragmento ha sido adaptado para reflejar nuestro enfoque de traducción de español a inglés. Los `encoder_inputs` son las oraciones en español y los `decoder_inputs` son las oraciones en inglés correspondientes. Esta estructura de datos está alineada con la configuración de nuestro modelo Transformer, donde el español actúa como entrada y el inglés como objetivo de la traducción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo rápido a las formas de las secuencias (tenemos lotes de 64 pares, y todas las secuencias tienen una longitud de 20 pasos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiZmGLluOf5n",
    "outputId": "1ddfeba7-98fb-49aa-dff8-5ba48d6740c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 20)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este fragmento de código nos permite verificar la estructura de los datos que estamos alimentando al modelo. Cada lote contiene 64 pares de oraciones, con las oraciones en español como `encoder_inputs` y las correspondientes oraciones en inglés (más los tokens `[start]` y `[end]`) como `decoder_inputs` y `targets`. Todas las secuencias están restringidas a una longitud de 20 pasos, lo que facilita el manejo uniforme de los datos en el modelo. Esta verificación es crucial para asegurarse de que los datos se estén preparando correctamente para el entrenamiento del modelo de traducción de español a inglés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción del Modelo\n",
    "\n",
    "Nuestro Transformer secuencia a secuencia consta de un `TransformerEncoder` y un `TransformerDecoder` encadenados. Para que el modelo sea consciente del orden de las palabras, también utilizamos una capa de `PositionalEmbedding`.\n",
    "\n",
    "La secuencia de origen (en español) se pasará al `TransformerEncoder`, que producirá una nueva representación de la misma. Esta nueva representación se pasará luego al `TransformerDecoder`, junto con la secuencia objetivo hasta el momento (palabras objetivo de 0 a N en inglés). El `TransformerDecoder` buscará predecir las siguientes palabras en la secuencia objetivo (N+1 y siguientes).\n",
    "\n",
    "Un detalle clave que hace esto posible es el enmascaramiento causal (véase el método `get_causal_attention_mask()` en `TransformerDecoder`). El `TransformerDecoder` ve las secuencias completas de una vez, por lo que debemos asegurarnos de que solo utilice información de los tokens objetivo de 0 a N al predecir el token N+1 (de lo contrario, podría utilizar información del futuro, lo que resultaría en un modelo que no se podría usar en el momento de la inferencia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "zwVsnlEJOf5n"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(dense_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, None, :], dtype=\"int32\")\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(0, length, 1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        else:\n",
    "            return tf.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(latent_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, None, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, None]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparación con el código original, hemos adaptado nuestro modelo para la traducción de español a inglés. Mantenemos la arquitectura general del Transformer, pero con la particularidad de que nuestras entradas y salidas están adaptadas para trabajar con secuencias en español como entradas y sus correspondientes traducciones en inglés como objetivos. Además, el manejo del enmascaramiento causal en el `TransformerDecoder` es crucial para garantizar que la predicción de cada palabra en inglés se base solo en las palabras anteriores, y no en las futuras, lo que es esencial para la efectividad del modelo durante la inferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, ensamblaremos el modelo completo de principio a fin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "739R6Ir-Of5o"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este paso, integramos los componentes del Transformer que hemos construido: el `TransformerEncoder`, el `TransformerDecoder` y la `PositionalEmbedding`. Los inputs del codificador son las secuencias en español, y los del decodificador son las secuencias en inglés hasta el momento. El objetivo es que el decodificador prediga la próxima palabra en la secuencia en inglés. Esta arquitectura refleja nuestra adaptación para el proyecto de traducción de español a inglés, asegurando que el modelo procese adecuadamente las secuencias de origen y genere las traducciones correspondientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de Nuestro Modelo\n",
    "\n",
    "Usaremos la precisión como una forma rápida de monitorear el progreso del entrenamiento en los datos de validación. Cabe destacar que, en la traducción automática, típicamente se utilizan puntuaciones BLEU y otras métricas, en lugar de la precisión.\n",
    "\n",
    "**Nota:** La puntuación BLEU (Bilingual Evaluation Understudy) es una métrica utilizada para evaluar la calidad de la traducción automática de textos. BLEU compara las traducciones generadas por la máquina con una o varias traducciones de referencia hechas por humanos, evaluando la similitud en términos de la precisión y fluidez de la traducción.\n",
    "\n",
    "BLEU funciona mediante el conteo de coincidencias de frases o n-gramas (secuencias de n palabras) entre la traducción generada y las traducciones de referencia. Luego, se calcula una puntuación que considera tanto la coincidencia de palabras como la longitud de la traducción, para evitar favorecer respuestas más cortas. Las puntuaciones BLEU suelen oscilar entre 0 y 100, donde un puntaje más alto indica una mayor similitud con las traducciones de referencia y, por lo tanto, se considera una mejor traducción.\n",
    "\n",
    "Aunque BLEU es ampliamente utilizada, tiene limitaciones, como la incapacidad para evaluar completamente la fluidez y la naturalidad del lenguaje. A pesar de esto, sigue siendo una herramienta estándar en la evaluación de modelos de traducción automática debido a su eficacia y facilidad de uso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kazw7xAHOf5o",
    "outputId": "be606cfb-a89a-47c4-97d7-e0c93571e550"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " positional_embedding (Posi  (None, None, 256)            3845120   ['encoder_inputs[0][0]']      \n",
      " tionalEmbedding)                                                                                 \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer  [(None, None)]               0         []                            \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n",
      " formerEncoder)                                                                                   \n",
      "                                                                                                  \n",
      " model_1 (Functional)        (None, None, 15000)          1295964   ['decoder_inputs[0][0]',      \n",
      "                                                          0          'transformer_encoder[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19960216 (76.14 MB)\n",
      "Trainable params: 19960216 (76.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "1302/1302 [==============================] - 104s 74ms/step - loss: 1.6882 - accuracy: 0.7453 - val_loss: 1.1472 - val_accuracy: 0.8186\n",
      "Epoch 2/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 1.0062 - accuracy: 0.8387 - val_loss: 0.8372 - val_accuracy: 0.8614\n",
      "Epoch 3/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.7885 - accuracy: 0.8678 - val_loss: 0.7284 - val_accuracy: 0.8773\n",
      "Epoch 4/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.6786 - accuracy: 0.8830 - val_loss: 0.6830 - val_accuracy: 0.8852\n",
      "Epoch 5/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.6065 - accuracy: 0.8932 - val_loss: 0.6571 - val_accuracy: 0.8890\n",
      "Epoch 6/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.5540 - accuracy: 0.9009 - val_loss: 0.6338 - val_accuracy: 0.8932\n",
      "Epoch 7/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.5117 - accuracy: 0.9072 - val_loss: 0.6280 - val_accuracy: 0.8952\n",
      "Epoch 8/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.4795 - accuracy: 0.9122 - val_loss: 0.6461 - val_accuracy: 0.8947\n",
      "Epoch 9/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.4516 - accuracy: 0.9167 - val_loss: 0.6203 - val_accuracy: 0.8986\n",
      "Epoch 10/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.4277 - accuracy: 0.9208 - val_loss: 0.6252 - val_accuracy: 0.8994\n",
      "Epoch 11/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.4070 - accuracy: 0.9240 - val_loss: 0.6491 - val_accuracy: 0.8985\n",
      "Epoch 12/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.3888 - accuracy: 0.9272 - val_loss: 0.6320 - val_accuracy: 0.9010\n",
      "Epoch 13/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.3723 - accuracy: 0.9301 - val_loss: 0.6466 - val_accuracy: 0.9002\n",
      "Epoch 14/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.3567 - accuracy: 0.9329 - val_loss: 0.6465 - val_accuracy: 0.9010\n",
      "Epoch 15/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.3424 - accuracy: 0.9355 - val_loss: 0.6534 - val_accuracy: 0.9013\n",
      "Epoch 16/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.3309 - accuracy: 0.9376 - val_loss: 0.6528 - val_accuracy: 0.9008\n",
      "Epoch 17/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.3189 - accuracy: 0.9397 - val_loss: 0.6705 - val_accuracy: 0.9016\n",
      "Epoch 18/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.3075 - accuracy: 0.9420 - val_loss: 0.6723 - val_accuracy: 0.9013\n",
      "Epoch 19/30\n",
      "1302/1302 [==============================] - 91s 70ms/step - loss: 0.2972 - accuracy: 0.9437 - val_loss: 0.6757 - val_accuracy: 0.9027\n",
      "Epoch 20/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2870 - accuracy: 0.9457 - val_loss: 0.6924 - val_accuracy: 0.9001\n",
      "Epoch 21/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2785 - accuracy: 0.9474 - val_loss: 0.6897 - val_accuracy: 0.9005\n",
      "Epoch 22/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.2709 - accuracy: 0.9489 - val_loss: 0.6997 - val_accuracy: 0.9018\n",
      "Epoch 23/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.2620 - accuracy: 0.9504 - val_loss: 0.7094 - val_accuracy: 0.9018\n",
      "Epoch 24/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2555 - accuracy: 0.9518 - val_loss: 0.7485 - val_accuracy: 0.9005\n",
      "Epoch 25/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2477 - accuracy: 0.9532 - val_loss: 0.7243 - val_accuracy: 0.9020\n",
      "Epoch 26/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2403 - accuracy: 0.9546 - val_loss: 0.7283 - val_accuracy: 0.9022\n",
      "Epoch 27/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2333 - accuracy: 0.9557 - val_loss: 0.7350 - val_accuracy: 0.9020\n",
      "Epoch 28/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.2283 - accuracy: 0.9568 - val_loss: 0.7436 - val_accuracy: 0.9016\n",
      "Epoch 29/30\n",
      "1302/1302 [==============================] - 89s 69ms/step - loss: 0.2224 - accuracy: 0.9580 - val_loss: 0.7554 - val_accuracy: 0.9010\n",
      "Epoch 30/30\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 0.2173 - accuracy: 0.9591 - val_loss: 0.7588 - val_accuracy: 0.9023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7e6697976ad0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "# Ver el resumen del modelo\n",
    "transformer.summary()\n",
    "\n",
    "# Compilar el modelo\n",
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",  # Puedes experimentar con diferentes optimizadores como 'adam'\n",
    "    loss=\"sparse_categorical_crossentropy\",  # Esta es una elección común para clasificación\n",
    "    metrics=[\"accuracy\"]  # Medir la precisión durante el entrenamiento\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "transformer.fit(\n",
    "    train_ds,  # Conjunto de datos de entrenamiento\n",
    "    epochs=epochs,  # Número de épocas para el entrenamiento\n",
    "    validation_data=val_ds  # Conjunto de datos de validación para evaluar la convergencia\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados del entrenamiento del modelo Transformer para la tarea de traducción de español a inglés muestran una mejora progresiva en la precisión y una disminución en la pérdida tanto en el conjunto de entrenamiento como en el de validación a lo largo de las 30 épocas.\r\n",
    "\r\n",
    "- **Pérdida (Loss)**: La pérdida, medida con la función de pérdida de entropía cruzada categórica dispersa, disminuye consistentemente a medida que avanza el entrenamiento. Esto indica que el modelo está aprendiendo efectivamente a predecir la siguiente palabra en las secuencias en inglés basándose en las secuencias en español y las palabras objetivo anteriores.\r\n",
    "\r\n",
    "- **Precisión (Accuracy)**: La precisión aumenta con cada época, lo que sugiere que el modelo se está volviendo más hábil en la predicción correcta de la siguiente palabra en la secuencia. Sin embargo, es importante recordar que la precisión no es la métrica ideal para la traducción automática, ya que no captura completamente la calidad y fluidez de las traducciones. En escenarios de producción, se recomendaría usar métricas como BLEU para una evaluación más completa.\r\n",
    "\r\n",
    "- **Validación**: El rendimiento en el conjunto de validación es un indicador crucial de cómo el modelo generalizará a nuevos datos. Aunque la pérdida en la validación tiende a ser mayor que en el entrenamiento, la diferencia no es significativamente grande, lo que es una señal positiva de que no hay un sobreajuste significativo.\r\n",
    "\r\n",
    "En resumen, estos resultados muestran un progreso prometedor en el entrenamiento del modelo. Sin embargo, para una evaluación más completa y precisa de su capacidad de traducción, se recomienda realizar pruebas adicionales con métricas específicas de traducción automática y analizar ejemplos de traducciones generadas por el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decodificando Oraciones de Prueba\r\n",
    "\r\n",
    "Finalmente, veamos cómo traducir oraciones completamente nuevas del español al inglés. Para hacerlo, simplemente alimentamos al modelo con la oración en español vectorizada, así como el token objetivo `\"[start]\"`. Luego, generamos repetidamente el próximo token hasta que alcancemos el token `\"[end]\"`.\r\n",
    "\r\n",
    "Este proceso implica proporcionar al modelo la oración en español y pedirle que genere su traducción en inglés, comenzando con el token `\"[start]\"` y continuando hasta que el modelo predice el token `\"[end]\"`, que señala el fin de la oración en inglés. Este enfoque es típico en la inferencia secuencia a secuencia y nos permite ver cómo el modelo maneja oraciones que no ha visto durante el entrenamiento, demostrando su capacidad para generalizar y producir traducciones coherentes en situaciones de la vida real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-nT1JmhLOf5p",
    "outputId": "e7156698-dca5-4875-9018-dd96ff3a9ef5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: Por la mañana el aire es fresco.\n",
      "English: in the air is air is fresh air\n",
      "--------------------------------------------------\n",
      "Spanish: El nuevo gobierno tiene problemas financieros.\n",
      "English: he new government has trouble with financial problems\n",
      "--------------------------------------------------\n",
      "Spanish: Tom quiso decir lo que dijo.\n",
      "English: om meant what to say\n",
      "--------------------------------------------------\n",
      "Spanish: Él presta dinero con un alta tasa de interés.\n",
      "English: he pays money with a rate in interest\n",
      "--------------------------------------------------\n",
      "Spanish: Ella es superficial y materialista.\n",
      "English: he is shallow and say about it\n",
      "--------------------------------------------------\n",
      "Spanish: Somos los amigos de Tom.\n",
      "English: were toms friends\n",
      "--------------------------------------------------\n",
      "Spanish: Esto es fabuloso.\n",
      "English: his is wonderful\n",
      "--------------------------------------------------\n",
      "Spanish: Eso es todo lo que ella escribió.\n",
      "English: hats all she wrot\n",
      "--------------------------------------------------\n",
      "Spanish: ¿Cómo de lejos está tu casa?\n",
      "English: hows your house far from you\n",
      "--------------------------------------------------\n",
      "Spanish: Su pasatiempo era coleccionar monedas antiguas.\n",
      "English: her hobby was collecting old coins\n",
      "--------------------------------------------------\n",
      "Spanish: No eres joven.\n",
      "English: youre not young\n",
      "--------------------------------------------------\n",
      "Spanish: Esa camisa no combina con los pantalones.\n",
      "English: hat shirt doesnt go with the pants\n",
      "--------------------------------------------------\n",
      "Spanish: El mal tiempo no es impedimento.\n",
      "English: he bad weather is not really sa\n",
      "--------------------------------------------------\n",
      "Spanish: Tom parece estar nervioso.\n",
      "English: om seems to be nervous\n",
      "--------------------------------------------------\n",
      "Spanish: Puede que sea demasiado viejo.\n",
      "English: maybe its too ol\n",
      "--------------------------------------------------\n",
      "Spanish: La cuenta, por favor.\n",
      "English: he bill pleas\n",
      "--------------------------------------------------\n",
      "Spanish: El nombre de Tom sale en el sobre.\n",
      "English: oms name doesnt go out on the envelop\n",
      "--------------------------------------------------\n",
      "Spanish: ¿Cuándo se casó?\n",
      "English: when did it be marri\n",
      "--------------------------------------------------\n",
      "Spanish: Afloje usted un poco la venda.\n",
      "English: urn a bit on the bandag\n",
      "--------------------------------------------------\n",
      "Spanish: Él tiene la pieza grande para él solo.\n",
      "English: he has the room in his room to himself\n",
      "--------------------------------------------------\n",
      "Spanish: Quiero comer.\n",
      "English: i want to eat\n",
      "--------------------------------------------------\n",
      "Spanish: ¿Te has vuelto loco?\n",
      "English: have you lost your mi\n",
      "--------------------------------------------------\n",
      "Spanish: Siempre conduzco por debajo de la velocidad permitida.\n",
      "English: i am driving at full speed sp\n",
      "--------------------------------------------------\n",
      "Spanish: Un perro le mordió la pierna.\n",
      "English: dog bit her leg\n",
      "--------------------------------------------------\n",
      "Spanish: Vístete.\n",
      "English: get dress\n",
      "--------------------------------------------------\n",
      "Spanish: Vuelvo a las seis y media.\n",
      "English: ill return at half past six\n",
      "--------------------------------------------------\n",
      "Spanish: ¿Quiere usted hablar más despacio?\n",
      "English: do you want to speak more slowly\n",
      "--------------------------------------------------\n",
      "Spanish: Hay muchos puentes en esta ciudad.\n",
      "English: here are many bridges in this city\n",
      "--------------------------------------------------\n",
      "Spanish: ¿Es posible beber agua salada?\n",
      "English: it might drink water\n",
      "--------------------------------------------------\n",
      "Spanish: Odio planchar.\n",
      "English: i hate to iro\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = eng_vectorization.get_vocabulary()\n",
    "eng_index_lookup = dict(zip(range(len(eng_vocab)), eng_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = spa_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = eng_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = tf.argmax(predictions[0, i, :], axis=-1).numpy()\n",
    "        sampled_token = eng_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence.lstrip(\"[start] \").rstrip(\" [end]\")\n",
    "\n",
    "test_spa_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(30):\n",
    "    input_sentence = random.choice(test_spa_texts)\n",
    "    translated = decode_sequence(input_sentence)\n",
    "    print(f\"Spanish: {input_sentence}\")\n",
    "    print(f\"English: {translated}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de 30 épocas de entrenamiento, hemos obtenido resultados como los siguientes para la traducción de español a inglés:\n",
    "\n",
    "> Por la mañana el aire es fresco.\n",
    "> [start] In the air is air is fresh air [end]\n",
    "\n",
    "> El nuevo gobierno tiene problemas financieros.\n",
    "> [start] He new government has trouble with financial problems [end]\n",
    "\n",
    "> Tom quiso decir lo que dijo.\n",
    "> [start] Om meant what to say [end]\n",
    "\n",
    "> Él presta dinero con un alta tasa de interés.\n",
    "> [start] He pays money with a rate in interest [end]\n",
    "\n",
    "> Somos los amigos de Tom.\n",
    "> [start] Were toms friends [end]\n",
    "\n",
    "> Esto es fabuloso.\n",
    "> [start] His is wonderful [end]\n",
    "\n",
    "> Eso es todo lo que ella escribió.\n",
    "> [start] Hats all she wrot [end]\n",
    "\n",
    "> ¿Cómo de lejos está tu casa?\n",
    "> [start] Hows your house far from you [end]\n",
    "\n",
    "> Su pasatiempo era coleccionar monedas antiguas.\n",
    "> [start] Her hobby was collecting old coins [end]\n",
    "\n",
    "> No eres joven.\n",
    "> [start] Youre not young [end]\n",
    "\n",
    "Estos ejemplos muestran cómo el modelo ha aprendido a traducir oraciones del español al inglés. Aunque en algunos casos logra traducciones coherentes, en otros hay errores o traducciones literales que podrían mejorarse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis de los Resultados y Precisión:**\n",
    "\n",
    "Los resultados muestran una capacidad variada del modelo para generar traducciones coherentes. Algunas oraciones son traducidas de manera bastante precisa y fluida, mientras que otras presentan errores o incoherencias. Esto puede deberse a varios factores:\n",
    "\n",
    "1. **Limitaciones del Modelo**: Aunque el modelo ha aprendido patrones generales del lenguaje, todavía puede tener dificultades con estructuras gramaticales complejas, ambigüedades o palabras menos comunes.\n",
    "\n",
    "2. **Tamaño del Vocabulario y Longitud de Secuencia**: El tamaño del vocabulario y la longitud máxima de secuencia pueden limitar la capacidad del modelo para manejar una variedad más amplia de frases y estructuras.\n",
    "\n",
    "3. **Calidad del Conjunto de Datos**: Las inconsistencias o limitaciones en los datos de entrenamiento pueden afectar el rendimiento del modelo.\n",
    "\n",
    "4. **Necesidad de Ajuste Fino**: El modelo puede beneficiarse de un ajuste fino adicional, posiblemente con un conjunto de datos más grande o diverso, o mediante ajustes en los hiperparámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones:\n",
    "\n",
    "- El modelo muestra un rendimiento prometedor, pero hay margen de mejora en términos de precisión y fluidez de las traducciones.\n",
    "- Se podrían considerar estrategias adicionales para mejorar el rendimiento, como aumentar el tamaño del vocabulario, extender la longitud máxima de las secuencias o emplear técnicas de post-procesamiento.\n",
    "- Para una evaluación más completa, sería útil utilizar métricas específicas de traducción automática, como BLEU, para obtener una medida cuantitativa de la calidad de las traducciones. \n",
    "\n",
    "En resumen, aunque el modelo ha aprendido a traducir de manera efectiva en muchos casos, aún hay áreas de mejora que pueden abordarse para aumentar la calidad general de las traducciones generadas."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
